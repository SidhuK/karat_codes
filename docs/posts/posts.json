[
  {
    "path": "posts/tidymodels-knn-and-glm/",
    "title": "TidyModels - KNN & GLM",
    "description": "Working with TidyModels to generate KNN and GLM models for the palmer penguins dataset.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-24",
    "categories": [
      "TidyModels",
      "KNN",
      "GLM",
      "Machine Learning",
      "Classification Models"
    ],
    "contents": "\n\nContents\nTidyModels - KNN and GLM\nPalmer\nPenguins Time!!\nLoading Datasets, Libraries\nExplore data\nBuild models\nGLM\nKNN\nEvaluate\nmodels\nROC Curve comparing the two\nmodels.\n\nAdditional\nReading:\n\nTidyModels - KNN and GLM\nPalmer Penguins Time!!\n (Figure from @Allison_Horst)\nThis is part 3 of my TidyModels blogs. The first part1\ndealt with Random Forests and SVM while the second part2 I\ntried out PCA and UMAP using TidyModels packages.\nThis time, I will be trying to perform K-Nearest Neighbors and Linear\nModelling using TidyModels.\nLoading Datasets, Libraries\n\n\nlibrary(tidymodels)\nlibrary(tidyverse)\n\n\ndata(\"penguins\")\nglimpse(penguins)\n\n\nRows: 344\nColumns: 7\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Ad…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39…\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19…\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 46…\n$ sex               <fct> male, female, female, NA, female, male, fe…\n\npenguins <- na.omit(penguins)\n\n\n\nExplore data\nExploratory data analysis (EDA) is an important part of the modeling\nprocess3.I am using the default tidymodels\ntemplate so will be using the inbuilt EDA performace cells to check the\ndata.\n\n\npenguins %>%\n  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex, size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nWe see a clear relation between the sex of the penguins with and\ntheir bill depth. Further, we can also see a relation between the bill\nlength for each of the species looks difference wrt to the other.\nBuild models\nLet’s consider how to spend our data budget4:\ncreate training and testing sets\ncreate resampling folds from the training set\n\n\nset.seed(123)\npenguin_split <- initial_split(penguins, strata = sex)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\nset.seed(234)\npenguin_folds <- vfold_cv(penguin_train, strata = sex)\npenguin_folds\n\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [223/26]> Fold01\n 2 <split [223/26]> Fold02\n 3 <split [223/26]> Fold03\n 4 <split [224/25]> Fold04\n 5 <split [224/25]> Fold05\n 6 <split [224/25]> Fold06\n 7 <split [225/24]> Fold07\n 8 <split [225/24]> Fold08\n 9 <split [225/24]> Fold09\n10 <split [225/24]> Fold10\n\nLet’s create a model specification5 for each model we want\nto try:\nGLM\n\n\nglm_spec <-\n  logistic_reg() %>%\n  set_engine(\"glm\")\n\n\n\nKNN\n\n\nknn_spec <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\n\n\nTo set up your modeling code, consider using the parsnip addin6 or the usemodels7\npackage.\nNow let’s build a model workflow8 combining each model\nspecification with a data preprocessor:\n\n\npenguin_formula <- sex ~ .\n\nglm_wf    <- workflow(penguin_formula, glm_spec)\n\nknn_wf <- workflow(penguin_formula, knn_spec)\n\n\n\nIf your feature engineering needs are more complex than provided by a\nformula like sex ~ ., use a recipe9\nEvaluate models\nThese models have no tuning parameters so we can evaluate them as\nthey are. [Learn about tuning hyperparameters here10\n\n\ncontrl_preds <- control_resamples(save_pred = TRUE)\n\nglm_rs <- fit_resamples(\n  glm_wf,\n  resamples = penguin_folds,\n  control = contrl_preds\n)\n\nknn_rs <- fit_resamples(\n  knn_wf,\n  resamples = penguin_folds,\n  control = contrl_preds\n)\n\n\n\nHow did these two models compare?\n\n\ncollect_metrics(glm_rs)\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.916    10  0.0173 Preprocessor1_Model1\n2 roc_auc  binary     0.975    10  0.0105 Preprocessor1_Model1\n\ncollect_metrics(knn_rs)\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.912    10 0.0132  Preprocessor1_Model1\n2 roc_auc  binary     0.979    10 0.00697 Preprocessor1_Model1\n\nROC Curve comparing the two\nmodels.\n\n\nbind_rows(\n  collect_predictions(glm_rs) %>%\n    mutate(mod = \"glm\"),\n  collect_predictions(knn_rs) %>%\n    mutate(mod = \"knn\")\n) %>%\n  group_by(mod) %>%\n  roc_curve(sex, .pred_female) %>%\n  autoplot()\n\n\n\n\nThese models perform very similarly, so perhaps we would choose the\nsimpler, linear model. The function last_fit()\nfits one final time on the training data and evaluates\non the testing data. This is the first time we have used the testing\ndata.\n\n\nfinal_fitted <- last_fit(glm_wf, penguin_split)\ncollect_metrics(final_fitted)  ## metrics evaluated on the *testing* data\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.857 Preprocessor1_Model1\n2 roc_auc  binary         0.937 Preprocessor1_Model1\n\nThis object contains a fitted workflow that we can use for\nprediction.\n\n\nfinal_wf <- extract_workflow(final_fitted)\npredict(final_wf, penguin_test[59,])\n\n\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 male       \n\npenguin_test[59,]\n\n\n# A tibble: 1 × 7\n  species island bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>   <fct>           <dbl>         <dbl>             <int>\n1 Gentoo  Biscoe           46.5          14.8               217\n# … with 2 more variables: body_mass_g <int>, sex <fct>\n\nYou can save this fitted final_wf object to use later\nwith new data, for example with readr::write_rds().\nAdditional Reading:\nhttps://rstudio-pubs-static.s3.amazonaws.com/749128_01ee1cc430c348438b211f3db0ed6d0a.html\nhttps://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html\nhttps://www.tidymodels.org/learn/models/\nhttps://juliasilge.com/blog/palmer-penguins/\n\nhttps://karatsidhu.com/posts/tidymodels-svm-random-forests/↩︎\nhttps://karatsidhu.com/posts/tidymodels-pca-and-umap/↩︎\nhttps://www.tmwr.org/software-modeling.html#model-phases↩︎\nhttps://www.tmwr.org/splitting.html↩︎\nhttps://www.tmwr.org/models.html↩︎\nhttps://parsnip.tidymodels.org/reference/parsnip_addin.html↩︎\nhttps://usemodels.tidymodels.org/↩︎\nhttps://www.tmwr.org/workflows.html↩︎\nhttps://www.tidymodels.org/start/recipes/↩︎\nhttps://www.tidymodels.org/start/tuning/↩︎\n",
    "preview": "posts/tidymodels-knn-and-glm/images/logo.svg",
    "last_modified": "2022-05-23T21:43:45+05:30",
    "input_file": {}
  },
  {
    "path": "posts/epl-2022-final-table/",
    "title": "EPL 2022 Final Table",
    "description": "Using gt package to create a great looking table for the EPL Standings 2021-22.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "Data-Viz",
      "misc",
      "TidyVerse"
    ],
    "contents": "\n\nContents\nHow to Make\nbeautiful tables in R using gt()\nUsing\ngt package to create a great looking table for the EPL Standings\n2021-22\n\nLoading the required\npackages\nLoading the\ndata\nCombining the two tables,\nMaking the final table:\nSaving the table as png\n(optional)\nFinal Result:\n\nHow to Make\nbeautiful tables in R using gt()\nUsing\ngt package to create a great looking table for the EPL Standings\n2021-22\nGrammar of tables or gt() package, combined with the gtextras()\npackage provides an easy and simple way to make high quality tables in\nR.\nI wanted to practice the gt() package in R, and decided to document\nmy journey to making a great looking table of the final standings for\nthe English Premier League now that the season is over.\nIt’s been a season to forget as a Manchester United fan :( and I am\nhopeful for better performances next season under Ten Hag, but the\nfootball as a neutral has been very exciting to watch.I digress, let’s\nget back on to the coding part.\nCode to make this possible, along with a brief explanation of each\npart of the process:\nLoading the required\npackages\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(showtext)\nlibrary(RColorBrewer)\n\n\n\nThe main font I used for the title is similar to the PL official\nfont.\n\n\nshowtext_auto()\nsysfonts::font_add(\"Premier League\", regular = \"FontsFree-Net-Barclays-Premier-League.ttf\")\n\n\n\nThe data file was downloaded from the FBref database as a .csv, so\nthe loading and cleaning of data was a relatively easy process\nLoading the data\n\n\ntable <- read.csv(\"pl_table_gw_38.csv\")\n\n\nlogos <-\n    read.csv(\n        \"https://raw.githubusercontent.com/steodose/Club-Soccer-Forecasts/main/team_mapping.csv\"\n    )\n\n\n\nData to be cleaned and the form table needs to be in the form of a\nlist inside a data frame. Cleaning it up:\n\n\nlogos_cleaned <- logos |>\n    select(squad_fbref, url_logo_espn) |>\n    rename(Squad = \"squad_fbref\")\n\ntable <- table |>\n    select(-c(Goalkeeper, Notes, Attendance))\n\n\n\nCombining the two tables,\n\n\ntable2 <- inner_join(table,\n    logos_cleaned,\n    by = \"Squad\"\n) |>\n    relocate(Rk, url_logo_espn)\n\n\n\nMaking the final table:\n\n\nfinal_table <- table2 |>\n    rename(` ` = \"url_logo_espn\") |>\n    rename(Rank = \"Rk\") |>\n    rename(`Top Scorer` = \"Top.Team.Scorer\") |>\n    gt() |>\n    gt_color_rows(\n        columns = W,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$W), min(table2$W))\n    ) |>\n    gt_color_rows(\n        columns = D,\n        type = \"discrete\",\n        direction = -1,\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$D), min(table2$D))\n    ) |>\n    gt_color_rows(\n        columns = L,\n        type = \"discrete\",\n        direction = -1,\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$L), min(table2$L))\n    ) |>\n    gt_color_rows(\n        columns = GF,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$GF), min(table2$GF))\n    ) |>\n    gt_color_rows(\n        columns = GA,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        direction = -1,\n        domain = c(min(table2$GA), max(table2$GA))\n    ) |>\n    gt_color_rows(\n        columns = GD,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$GD), min(table2$GD))\n    ) |>\n    gt_color_rows(\n        columns = Pts,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$Pts), min(table2$Pts))\n    ) |>\n    gt_color_rows(\n        xG,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$xG), min(table2$xG))\n    ) |>\n    gt_color_rows(\n        xGA,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        direction = -1,\n        domain = c(min(table2$xGA), max(table2$xGA))\n    ) |>\n    gt_color_rows(\n        xGD,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$xGD), min(table2$xGD))\n    ) |>\n    gt_color_rows(\n        xGD.90,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$xGD.90), min(table2$xGD.90))\n    ) |>\n    gt_img_rows(\n        columns = ` `,\n        img_source = \"web\",\n        height = 35\n    ) |>\n    tab_style(\n        style = list(cell_text(weight = \"bold\")),\n        locations = cells_body(rows = Squad == \"Manchester Utd\")\n    ) |>\n    gt_theme_espn() |>\n    tab_style(\n        style = list(cell_text(weight = \"bold\")),\n        locations = cells_body(\n            columns = Squad\n        )\n    ) |>\n    tab_header(\n        title = \"2021-22 Premier League Standings\",\n        subtitle = \"Final Results for the English Premier League saw Manchester City win the league after an exciting last few weeks of competition\n    with Liverpool. The competition came down to the final few minutes of the season where City were crowned winners.\n    The top 4 teams qualified for the UCL, 5th and 6th place teams are in the Europa League while the 7th place West Ham\n    secured the UECL qualification. The bottom 3 teams, Norwich, Watford and Burnley were relegated to the English Championship (2nd tier competition);\n    Everton and Leeds United narrowly escaped relegation.\"\n    ) |>\n    tab_footnote(\n        footnote = \"xG = Expected Goals (Higher is better)\",\n        locations = cells_column_labels(columns = xG)\n    ) |>\n    tab_footnote(\n        footnote = \"xGA = Expect Goals Against (Lower is better) \",\n        locations = cells_column_labels(columns = xGA)\n    ) |>\n    tab_footnote(\n        footnote = \"xGD = xG Difference (xG - xGA (High is better))\",\n        locations = cells_column_labels(columns = xGD)\n    ) |>\n    tab_footnote(\n        footnote = \"xGD.90 = xG Difference per 90 mins (Higher is better)\",\n        locations = cells_column_labels(columns = xGD.90)\n    ) |>\n    tab_footnote(\n        footnote = \"The team that I support :(\",\n        locations = cells_body(\n            columns = Squad,\n            rows = 6\n        )\n    ) |>\n    tab_source_note(\n        source_note = \"Data : FBRef | Table : Github.com/SidhuK\"\n    ) |>\n    tab_style(\n        locations = cells_title(groups = \"title\"), # format the main title\n        style = list(\n            cell_text(\n                font = \"Premier League\",\n                size = px(40),\n                color = \"black\",\n                weight = 700\n            )\n        )\n    ) |>\n    tab_options(\n        table.background.color = \"#eeeeee\",\n        column_labels.background.color = \"#eeeeee\"\n    ) # set the bg color\n\n\n\nSaving the table as png\n(optional)\n(Use gt save to save it as a png and not an html object)\nFinal Result:\n\n\nfinal_table\n\n\n\n2021-22 Premier League Standings\n    Final Results for the English Premier League saw Manchester City win the league after an exciting last few weeks of competition\n    with Liverpool. The competition came down to the final few minutes of the season where City were crowned winners.\n    The top 4 teams qualified for the UCL, 5th and 6th place teams are in the Europa League while the 7th place West Ham\n    secured the UECL qualification. The bottom 3 teams, Norwich, Watford and Burnley were relegated to the English Championship (2nd tier competition);\n    Everton and Leeds United narrowly escaped relegation.\n    Rank\n       \n      Squad\n      MP\n      W\n      D\n      L\n      GF\n      GA\n      GD\n      Pts\n      xG1\n      xGA2\n      xGD3\n      xGD.904\n      Top Scorer\n    1\n\nManchester City\n38\n29\n6\n3\n99\n26\n73\n93\n86.1\n26.9\n59.3\n1.60\nKevin De Bruyne - 152\n\nLiverpool\n38\n28\n8\n2\n94\n26\n68\n92\n86.1\n33.1\n52.9\n1.43\nMohamed Salah - 233\n\nChelsea\n38\n21\n11\n6\n76\n33\n43\n74\n64.8\n34.9\n29.9\n0.81\nMason Mount - 114\n\nTottenham\n38\n22\n5\n11\n69\n40\n29\n71\n61.5\n39.0\n22.5\n0.61\nSon Heung-min - 235\n\nArsenal\n38\n22\n3\n13\n61\n48\n13\n69\n56.8\n46.0\n10.8\n0.29\nBukayo Saka - 116\n\nManchester Utd5\n38\n16\n10\n12\n57\n57\n0\n58\n54.0\n54.1\n-0.1\n0.00\nCristiano Ronaldo - 187\n\nWest Ham\n38\n16\n8\n14\n60\n51\n9\n56\n49.6\n49.3\n0.3\n0.01\nJarrod Bowen - 128\n\nLeicester City\n38\n14\n10\n14\n62\n59\n3\n52\n48.5\n59.8\n-11.3\n-0.30\nJamie Vardy - 159\n\nBrighton\n38\n12\n15\n11\n42\n44\n-2\n51\n44.4\n45.2\n-0.9\n-0.02\nLeandro Trossard Neal Maupay - 810\n\nWolves\n38\n15\n6\n17\n38\n43\n-5\n51\n34.9\n57.1\n-22.3\n-0.60\nRaúl Jiménez - 611\n\nNewcastle Utd\n38\n13\n10\n15\n44\n62\n-18\n49\n38.5\n54.0\n-15.5\n-0.42\nCallum Wilson - 812\n\nCrystal Palace\n38\n11\n15\n12\n50\n46\n4\n48\n45.9\n40.1\n5.7\n0.16\nWilfried Zaha - 1413\n\nBrentford\n38\n13\n7\n18\n48\n56\n-8\n46\n46.7\n47.7\n-1.0\n-0.03\nIvan Toney - 1214\n\nAston Villa\n38\n13\n6\n19\n52\n54\n-2\n45\n43.7\n46.2\n-2.5\n-0.07\nOllie Watkins - 1115\n\nSouthampton\n38\n9\n13\n16\n43\n67\n-24\n40\n45.6\n56.3\n-10.7\n-0.29\nJames Ward-Prowse - 1016\n\nEverton\n38\n11\n6\n21\n43\n66\n-23\n39\n41.6\n51.4\n-9.8\n-0.26\nRicharlison - 1017\n\nLeeds United\n38\n9\n11\n18\n42\n79\n-37\n38\n45.8\n69.0\n-23.2\n-0.63\nRaphael Dias Belloli - 1118\n\nBurnley\n38\n7\n14\n17\n34\n53\n-19\n35\n37.5\n54.9\n-17.4\n-0.47\nMaxwel Cornet - 919\n\nWatford\n38\n6\n5\n27\n34\n77\n-43\n23\n37.8\n64.2\n-26.4\n-0.71\nEmmanuel Dennis - 1020\n\nNorwich City\n38\n5\n7\n26\n23\n84\n-61\n22\n33.0\n73.3\n-40.3\n-1.09\nTeemu Pukki - 11Data : FBRef | Table : Github.com/SidhuK\n    1 xG = Expected Goals (Higher is better)\n    2 xGA = Expect Goals Against (Lower is better) \n    3 xGD = xG Difference (xG - xGA (High is better))\n    4 xGD.90 = xG Difference per 90 mins (Higher is better)\n    5 The team that I support :(\n    \n\n\n\n\n",
    "preview": "posts/epl-2022-final-table/images/logo.png",
    "last_modified": "2022-05-23T21:32:39+05:30",
    "input_file": {},
    "preview_width": 3000,
    "preview_height": 2000
  },
  {
    "path": "posts/tidymodels-pca-and-umap/",
    "title": "TidyModels - PCA and UMAP",
    "description": "Working with TidyModels to generate PCA and UMAP for Cocktail recipes. Using these\nrecipes extrapolating the relation of a cocktail to each other.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-22",
    "categories": [
      "TidyModels",
      "PCA",
      "UMAP",
      "Machine Learning",
      "Dimension Reduction"
    ],
    "contents": "\n\nContents\nTidyModels - PCA & UMAP\nDimensionality\nReduction analysis with Tidymodels\n\nCocktail Recipes Dataset\nLoading\npackages\nLoading Data\nData Cleaning and\nExploratory Data Analysis\nMost Common Ingredients\nColumn Cleaning and data\nconversion\nNew Dataframe with a “wide”\nformat\n\nPrincipal Component Analysis\nPrepare the analysis\nRecipe code\nPrep Code\n\nPCA Table\nPCA Plots\nPC1 and PC2 main\ncontributors\nPCA Plot\n\n\nUniform Manifold\nAutomation & Projection\nRecipe Prep\n\nConclusion\nAdditional\nReading:\n\nTidyModels - PCA & UMAP\nDimensionality\nReduction analysis with Tidymodels\nNote: This is part 2 of the tidymodels practise. A few of\nthese RMarkdown files were generated on a date different to the\npublishing date.\nPrincipal Component analysis (and UMAP to a lesser extent) have been\nextensively used in Biological Sciences and specifically in Omics.\nCompared to other components of the TidyVerse, I am fairly comfortable\nusing and running the PCA test on a set of data. However, I typically\nuse either the base R (prcom) or PCA Tools1\nbecause generating a series of plots using those packages is fairly\nstraightforward2.\nCocktail Recipes Dataset\nThe dataset used is from the Tidytuesday data, where different\ncocktails and the components used for their recipes is provided.\nReducing dimensions in such a case would help in finding out which\ncocktail is closely related to the other in terms of their\ncomponents.\nLoading packages\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tidytext)\n\n\n\nLoading Data\n\n\nboston_cocktails <-\n  readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/boston_cocktails.csv\")\n\n\n\nData Cleaning and\nExploratory Data Analysis\nPreliminary look at the data\n\n\nboston_cocktails\n\n\n# A tibble: 3,643 × 6\n   name            category row_id ingredient_numb… ingredient measure\n   <chr>           <chr>     <dbl>            <dbl> <chr>      <chr>  \n 1 Gauguin         Cocktai…      1                1 Light Rum  2 oz   \n 2 Gauguin         Cocktai…      1                2 Passion F… 1 oz   \n 3 Gauguin         Cocktai…      1                3 Lemon Jui… 1 oz   \n 4 Gauguin         Cocktai…      1                4 Lime Juice 1 oz   \n 5 Fort Lauderdale Cocktai…      2                1 Light Rum  1 1/2 …\n 6 Fort Lauderdale Cocktai…      2                2 Sweet Ver… 1/2 oz \n 7 Fort Lauderdale Cocktai…      2                3 Juice of … 1/4 oz \n 8 Fort Lauderdale Cocktai…      2                4 Juice of … 1/4 oz \n 9 Apple Pie       Cordial…      3                1 Apple sch… 3 oz   \n10 Apple Pie       Cordial…      3                2 Cinnamon … 1 oz   \n# … with 3,633 more rows\n\nSo the PCA should be carried out for the “name” field and looking at\nthe relation between the cocktail and the incredients, category and the\nmeasured amount of each ingredient used.\nMost Common Ingredients\n\n\nboston_cocktails |>\n  count(ingredient, sort = TRUE)\n\n\n# A tibble: 569 × 2\n   ingredient            n\n   <chr>             <int>\n 1 Gin                 176\n 2 Fresh lemon juice   138\n 3 Simple Syrup        115\n 4 Vodka               114\n 5 Light Rum           113\n 6 Dry Vermouth        107\n 7 Fresh Lime Juice    107\n 8 Triple Sec          107\n 9 Powdered Sugar       90\n10 Grenadine            85\n# … with 559 more rows\n\nA lot of cocktails look like are using Gin, Lemon Juice, Syrup and\nVodka as some of the most common ingredients.\nColumn Cleaning and data\nconversion\nThe data isn’t very clean yet. Some ingredients, which are clearly\nthe same are labelled slightly differently. So we need to clean and make\nthe data usable for our analysis.\nSome of the main changes needed in the\ningredients:\nturn all ingredients to lowercase\n“-” needs to be chagned to ” ”\n“lemon” to “lemon Juice”\n“lime” to “lime juice”\n“grapefruit” to “grapefruit juice” (same for orange)\nSome of the main changes needed in the measure:\n1/2 to 0.5\n3/4 to 0.75\n1/4 to 0.25\nremove words like “dash”, and “oz” etc.\nConvert the entire column into a number\n\n\ncocktails <- boston_cocktails |> \n  mutate(\n    ingredient = str_to_lower(ingredient),\n    ingredient = str_replace_all(ingredient, \"-\", \" \"),\n    ingredient = str_remove(ingredient, \" liqueur$\"),\n    ingredient = str_remove(ingredient, \" (if desired)$\"),\n    ingredient = case_when(\n      str_detect(ingredient, \"bitters\") ~ \"bitters\",\n      str_detect(ingredient, \"lemon\") ~ \"lemon juice\",\n      str_detect(ingredient, \"lime\") ~ \"lime juice\",\n      str_detect(ingredient, \"grapefruit\") ~ \"grapefruit juice\",\n      str_detect(ingredient, \"orange\") ~ \"orange juice\",\n      TRUE ~ ingredient\n    ),\n    measure = case_when(\n      str_detect(ingredient, \"bitters\") ~ str_replace(measure, \"oz$\", \"dash\"),\n      TRUE ~ measure\n    ),\n    measure = str_replace(measure, \" ?1/2\", \".5\"),\n    measure = str_replace(measure, \" ?3/4\", \".75\"),\n    measure = str_replace(measure, \" ?1/4\", \".25\"),\n    measure_number = parse_number(measure),\n    measure_number = if_else(str_detect(measure, \"dash$\"), # a few drops = so 0.02 oz\n      measure_number / 50,\n      measure_number\n    )\n  ) |> \n  add_count(ingredient) |> \n  filter(n > 15) |> \n  select(-n) |> \n  distinct(row_id, ingredient, .keep_all = TRUE) |> \n  na.omit()\n\n\ncocktails\n\n\n# A tibble: 2,542 × 7\n   name            category row_id ingredient_numb… ingredient measure\n   <chr>           <chr>     <dbl>            <dbl> <chr>      <chr>  \n 1 Gauguin         Cocktai…      1                1 light rum  2 oz   \n 2 Gauguin         Cocktai…      1                3 lemon jui… 1 oz   \n 3 Gauguin         Cocktai…      1                4 lime juice 1 oz   \n 4 Fort Lauderdale Cocktai…      2                1 light rum  1.5 oz \n 5 Fort Lauderdale Cocktai…      2                2 sweet ver… .5 oz  \n 6 Fort Lauderdale Cocktai…      2                3 orange ju… .25 oz \n 7 Fort Lauderdale Cocktai…      2                4 lime juice .25 oz \n 8 Cuban Cocktail… Cocktai…      4                1 lime juice .5 oz  \n 9 Cuban Cocktail… Cocktai…      4                2 powdered … .5 oz  \n10 Cuban Cocktail… Cocktai…      4                3 light rum  2 oz   \n# … with 2,532 more rows, and 1 more variable: measure_number <dbl>\n\nNew Dataframe with a “wide”\nformat\nWe’re now close to beginning our analysis. However, the data is in\nlong format and PCA prefers to get the data wider. So we’ll pivot_wide\nto make the analysis easier.\n\n\ncocktails_df <- cocktails |> \n  select(-ingredient_number, -row_id, -measure) |> \n  pivot_wider(names_from = ingredient, values_from = measure_number, values_fill = 0) |> \n  janitor::clean_names() |> \n  na.omit()\n\ncocktails_df\n\n\n# A tibble: 937 × 42\n   name       category light_rum lemon_juice lime_juice sweet_vermouth\n   <chr>      <chr>        <dbl>       <dbl>      <dbl>          <dbl>\n 1 Gauguin    Cocktai…      2           1          1               0  \n 2 Fort Laud… Cocktai…      1.5         0          0.25            0.5\n 3 Cuban Coc… Cocktai…      2           0          0.5             0  \n 4 Cool Carl… Cocktai…      0           0          0               0  \n 5 John Coll… Whiskies      0           1          0               0  \n 6 Cherry Rum Cocktai…      1.25        0          0               0  \n 7 Casa Blan… Cocktai…      2           0          1.5             0  \n 8 Caribbean… Cocktai…      0.5         0          0               0  \n 9 Amber Amo… Cordial…      0           0.25       0               0  \n10 The Joe L… Whiskies      0           0.5        0               0  \n# … with 927 more rows, and 36 more variables: orange_juice <dbl>,\n#   powdered_sugar <dbl>, dark_rum <dbl>, cranberry_juice <dbl>,\n#   pineapple_juice <dbl>, bourbon_whiskey <dbl>, simple_syrup <dbl>,\n#   cherry_flavored_brandy <dbl>, light_cream <dbl>,\n#   triple_sec <dbl>, maraschino <dbl>, amaretto <dbl>,\n#   grenadine <dbl>, apple_brandy <dbl>, brandy <dbl>, gin <dbl>,\n#   anisette <dbl>, dry_vermouth <dbl>, …\n\nThis will be the starting dataset used to run PCA (and UMAP)\nPrincipal Component Analysis\nHenceforth I will be using CMDLineTips Post3 as\na reference because I’ve never done a PCA with TidyModels before.\nPrepare the analysis\nRecipe code\n\n\npca_rec <- recipe(~., data = cocktails_df) |>  # what data to use\n  update_role(name, category, new_role = \"id\") |> # name and category are identifiers not variables\n  step_normalize(all_predictors()) |> # normalize all other columns\n  step_pca(all_predictors()) # pca for all other columns\n\n\n\nPrep Code\n\n\npca_prep <- prep(pca_rec)\n\npca_prep\n\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          2\n predictor         40\n\nTraining data contained 937 data points and no missing data.\n\nOperations:\n\nCentering and scaling for light_rum, lemon_juice, lime_juice, s... [trained]\nPCA extraction with light_rum, lemon_juice, lime_juice, sw... [trained]\n\nThe prep object now doesn’t contain any output, and we need to tidy\nthe object to read it. List object contains the PCA “results” and\ncomponents on the 2nd place under “term_info” (open it in the console to\nsee details).\nPCA Table\n\n\ntidied_pca <- tidy(pca_prep, 2)\n\ntidied_pca\n\n\n# A tibble: 1,600 × 4\n   terms             value component id       \n   <chr>             <dbl> <chr>     <chr>    \n 1 light_rum        0.163  PC1       pca_uR86f\n 2 lemon_juice     -0.0140 PC1       pca_uR86f\n 3 lime_juice       0.224  PC1       pca_uR86f\n 4 sweet_vermouth  -0.0661 PC1       pca_uR86f\n 5 orange_juice     0.0308 PC1       pca_uR86f\n 6 powdered_sugar  -0.476  PC1       pca_uR86f\n 7 dark_rum         0.124  PC1       pca_uR86f\n 8 cranberry_juice  0.0954 PC1       pca_uR86f\n 9 pineapple_juice  0.119  PC1       pca_uR86f\n10 bourbon_whiskey  0.0963 PC1       pca_uR86f\n# … with 1,590 more rows\n\nPCA Plots\nA lot of Principal components seen, plotting the first 4\n\n\ntidied_pca |> \n  filter(\n    component == \"PC1\" |\n      component == \"PC2\" |\n      component == \"PC3\" |\n      component == \"PC4\"\n   ) |> \n  mutate(component = fct_inorder(component)) |> \n    ggplot(aes(value, terms, fill = terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  hrbrthemes::theme_ipsum() +\n  labs(y = NULL) \n\n\n\n\nLooking at the PC1, simple syrup and powdered sugar are very\ndifferent from each other, therefore the cocktails must be using one or\nthe other. They also appear to be the two biggest factors.\nPC1 and PC2 main\ncontributors\n\n\ntidied_pca |> \n  filter(component %in% paste0(\"PC\", 1:2)) |> \n  group_by(component) |>\n  top_n(8, abs(value)) |>\n  ungroup() |>\n  mutate(terms = reorder_within(terms, abs(value), component)) |>\n  ggplot(aes(abs(value), terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Positive?\"\n  ) +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nPCA Plot\n\n\njuice(pca_prep) |> \n  ggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  ggrepel::geom_text_repel(max.overlaps = 10) +\n  labs(color = NULL) + hrbrthemes::theme_ipsum()\n\n\n\n\nSimilar kind of cocktails are clustered together, so PCA analysis\nappears to be somewhat successful.\nUniform Manifold\nAutomation & Projection\nRecipe Prep\nThe embed package provides recipe steps for ways to create embeddings\nincluding UMAP.\n\n\nlibrary(embed)\n\numap_rec <- recipe(~., data = cocktails_df) |>\n  update_role(name, category, new_role = \"id\") |>\n  step_normalize(all_predictors()) |>\n  step_umap(all_predictors())\n\numap_prep <- prep(umap_rec)\n\numap_prep\n\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          2\n predictor         40\n\nTraining data contained 937 data points and no missing data.\n\nOperations:\n\nCentering and scaling for light_rum, lemon_juice, lime_juice, s... [trained]\nUMAP embedding for light_rum, lemon_juice, lime_juice, ... [trained]\n\nSo far, the process is exactly the same as PCA. Prep the recipe and\nget the “non output”-output.\n\n\njuice(umap_prep) |> \n  ggplot(aes(UMAP1, UMAP2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  ggrepel::geom_text_repel(max.overlaps = 30) +\n  labs(color = NULL) + hrbrthemes::theme_ipsum()\n\n\n\n\nSimilarly clustering seen here, but type of cocktails seen here are\ndifferent.\nConclusion\nThe PCA (and UMAP) using TidyModels offers a bit more customization\ncompared to using a regular prcomp or\nPCA-Tools. However, it requires a bit more code and felt\nslightly slower in running compared to the other packages (without\nactually testing the time fyi)4.\nAdditional Reading:\nEasily the best explanation of PCA on the internet: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\nHow PCA works - interactive plots: https://setosa.io/ev/\nHow UMAP works: https://umap-learn.readthedocs.io/en/latest/how_umap_works.html\n\nhttps://bioconductor.org/packages/release/bioc/html/PCAtools.html↩︎\nhttps://github.com/SidhuK/R_for_Metabolomics/tree/main/Principal_Component_Analysis↩︎\nhttps://cmdlinetips.com/2020/06/pca-with-tidymodels-in-r/↩︎\nPlots were slower to render↩︎\n",
    "preview": "posts/tidymodels-pca-and-umap/images/logo.svg",
    "last_modified": "2022-05-22T23:42:08+05:30",
    "input_file": {}
  },
  {
    "path": "posts/tidymodels-svm-random-forests/",
    "title": "TidyModels - SVM & Random Forests",
    "description": "Learning Tidymodels package using the Chocolates dataset from TidyTuesday.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-22",
    "categories": [
      "TidyModels",
      "Random Forest",
      "SVM",
      "Machine Learning",
      "Text-analysis"
    ],
    "contents": "\n\nContents\nTidyModels\nText Prediction using Random Forests and SVM\nChocolate\nRatings\nGoal\nLoading the\ndata\n\nExploratory Data Analysis\nLets see how the\nratings are distributed\nTidyText words analysis\nRating vs Word\nrelationship\n\n\nModel Building\nBuilding Models with\nTidymodels\nTokenization\nModel\nSpecification\nRandom\nForest Model\nSVM Model\n\n\nModel\nEvaluation\nHow did these two models\ncompare?\nSVM\nRandom\nForest\n\nChoosing a\nmodel\nRating Bais Visualization\n\n\nTidyModels\nText Prediction using Random Forests and SVM\nThe purpose of this post is for me to learn more about tidymodels\npackage, as well as learning and deploying models for prediction. This\nis hopefully a first in the series of many posts where I try and learn\nmore about various algorithms that are present in this package.\nChocolate Ratings\nThe dataset used today will be from the TidyTuesday data1. Since I am almost a complete\nbeginner I will be making use of Julie Silge’s great blogs2 to\nlearn more about how to use and run the models .\nGoal\nThe goal of this first exercise is to learn more about text analysis\nand using various reviews from chocolates to predict the ratings for a\nparticular chocolate bar.\nThis is a very vague and non specific way of predicting the outcome\nbut a good starting point in learning how the algorithms work. ##\nLoading the packages\n\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tidymodels)\nlibrary(textrecipes)\n\n\n\nLoading the data\n\n\nchocolate <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv')\n\n\n\nExploratory Data Analysis\nLets see how the\nratings are distributed\n\n\nchocolate |> \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 12) +\n  theme_minimal() +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nFrom the chart above, it looks like that most chocolates are rated\nsomewhere in the range of 2.5 and 3.75 with a few high-rated and\nlow-rated exceptions.\nSo we’ll be comparing the ratings and the descriptive words used to\ndescribe the corresponding ratings.\nTidyText words analysis\nLets use the tidytext() library to check what are some of the most\ncommon words used to describe the flavor of each chocolate in the\ndata-set.\n\n\n# split the characteristics column into words using tidytext, and make a new column called word instead of the original.\n\ntidy_chocolate <-\n  chocolate %>%\n  unnest_tokens(word, most_memorable_characteristics)\n\ntidy_chocolate |> \n  group_by(word) |> \n  summarise(total = n()) |> arrange(desc(total))\n\n\n# A tibble: 547 × 2\n   word    total\n   <chr>   <int>\n 1 cocoa     419\n 2 sweet     318\n 3 nutty     278\n 4 fruit     273\n 5 roasty    228\n 6 mild      226\n 7 sour      208\n 8 earthy    199\n 9 creamy    189\n10 intense   178\n# … with 537 more rows\n\nIt looks like the usual expected words like cocoa, sweet, nutty etc\nare the most prevalent.\nSince we know what the most common words are, its time to look at how\nan average chocolate described by these words is rated.\nRating vs Word relationship\n\n\ntidy_chocolate |> \n  group_by(word) |> \n  summarise(\n    n = n(),\n    rating = mean(rating) \n  ) |> \n  ggplot(aes(n, rating)) +\n  geom_jitter(color = \"maroon\", alpha = 0.7) +\n   geom_hline(\n    yintercept = mean(chocolate$rating), lty = 2,\n    color = \"gray50\", size = 1.5\n  ) + ggrepel::geom_text_repel(aes(label = word), max.overlaps = 15) +\n  scale_x_log10() +\n  theme_minimal() +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nWords like chemical, burnt, medicinal, pastey, bitter etc look to be\nassociated with generally low rated chocolate, while cocoa, complex,\ncreamy, balanced etc are higher rated chocolates.3\nWe now have a bit of an idea of what the general feeling of the data\nrating, and we can go on to buiding the models.\nModel Building\nBuilding Models with\nTidymodels\nLet’s start our modeling by setting up our “data budget.” We’ll\nstratify by our outcome “rating” which is what we want to measure using\nthe tokens.\n\n\nlibrary(tidymodels)\n\n\n\nTime to split the data into training and testing data\n\n\nset.seed(123)\nchoco_split <- initial_split(chocolate, strata = rating)\nchoco_train <- training(choco_split)\nchoco_test <- testing(choco_split)\n\n\n\ncreate resampling folds from the training set\n\n\nset.seed(234)\nchoco_folds <- vfold_cv(choco_train, strata = rating)\nchoco_folds\n\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [1705/191]> Fold01\n 2 <split [1705/191]> Fold02\n 3 <split [1705/191]> Fold03\n 4 <split [1706/190]> Fold04\n 5 <split [1706/190]> Fold05\n 6 <split [1706/190]> Fold06\n 7 <split [1707/189]> Fold07\n 8 <split [1707/189]> Fold08\n 9 <split [1708/188]> Fold09\n10 <split [1709/187]> Fold10\n\nWe’re done with splitting the data into test and train, and we’re\nusing the training data to train the model. So the first step will\ninvolve setting up feature engineering. The data right now is complex\nand we need to transform it into features that are useful for our model\ntokenization and computing.\nTokenization\n(if that’s a word?)\nWe’ll use textrecipes package to tokenize\n“most_memorable_characteristics” wrt “ratings” and look at the 100 most\ncommon words used (here they are called tokens).All of this is done on\nthe\n\n\nlibrary(textrecipes)\n\nchoco_rec <-\n  recipe(rating ~ most_memorable_characteristics, data = choco_train) %>%\n  step_tokenize(most_memorable_characteristics) %>%\n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% # 100 most common words\n  step_tfidf(most_memorable_characteristics) # step frequeence df\n\n\n# looking at the tokenized data\nprep(choco_rec) %>% bake(new_data = NULL)\n\n\n# A tibble: 1,896 × 101\n   rating tfidf_most_memorable_char… tfidf_most_memo… tfidf_most_memo…\n    <dbl>                      <dbl>            <dbl>            <dbl>\n 1   3                          0                   0                0\n 2   2.75                       0                   0                0\n 3   3                          0                   0                0\n 4   3                          0                   0                0\n 5   2.75                       0                   0                0\n 6   3                          1.38                0                0\n 7   2.75                       0                   0                0\n 8   2.5                        0                   0                0\n 9   2.75                       0                   0                0\n10   3                          0                   0                0\n# … with 1,886 more rows, and 97 more variables:\n#   tfidf_most_memorable_characteristics_banana <dbl>,\n#   tfidf_most_memorable_characteristics_base <dbl>,\n#   tfidf_most_memorable_characteristics_basic <dbl>,\n#   tfidf_most_memorable_characteristics_berry <dbl>,\n#   tfidf_most_memorable_characteristics_bitter <dbl>,\n#   tfidf_most_memorable_characteristics_black <dbl>, …\n\nThe result is basically a new dataframe from the “choco_train” data\nwith all the ratings, and the frequency of corresponding 100 most common\nwords.\nModel Specification\nThe models being used to evaluate the data are random forest, and\nsupport vector machine (SVM).\nRandom Forest Model is usually not great for text based or language\ndata4 and SVM normally a good model to use\nfor such data, so we’ll be trying both.\nRandom Forest Model\nSpecifying the RF model\nComputational engine: ranger (default)\n\n\n### Computational engine: ranger\n\nrf_spec <-\n  rand_forest(trees = 500) %>%\n  set_mode(\"regression\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 500\n\nComputational engine: ranger \n\nSVM Model\nModel Specification\nComputational engine: LiblineaR (default)\n\n\nsvm_spec <-\n  svm_linear() %>%\n  set_mode(\"regression\")\n\nsvm_spec\n\n\nLinear Support Vector Machine Specification (regression)\n\nComputational engine: LiblineaR \n\nThe models have been specified and now we can run each of them in our\nworkflow().\nNote: The SVM requires the predictors to all be on the same scale5, but all our predictors are now\ntf-idf values so we should be pretty much fine.\n\n\nsvm_wf <- workflow(choco_rec, svm_spec)\nrf_wf <- workflow(choco_rec, rf_spec)\n\n\n\nWe are done with making the models and now can evaluate both of\nthem.\nModel Evaluation\nThese workflows have no tuning parameters so we can evaluate them as\nthey are. (Random forest models can be tuned but they tend to work fine\nwith the defaults as long as you have enough trees.)\n\n\ndoParallel::registerDoParallel() # run them in parallel\n\ncontrl_preds <- control_resamples(save_pred = TRUE)\n\nsvm_rs <- fit_resamples(\n  svm_wf,\n  resamples = choco_folds,\n  control = contrl_preds\n)\n\nranger_rs <- fit_resamples(\n  rf_wf,\n  resamples = choco_folds,\n  control = contrl_preds\n)\n\n\n\nHow did these two models\ncompare?\nSVM\n\n\ncollect_metrics(svm_rs)\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.347    10 0.00656 Preprocessor1_Model1\n2 rsq     standard   0.367    10 0.0181  Preprocessor1_Model1\n\nRandom Forest\n\n\ncollect_metrics(ranger_rs)\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.350    10 0.00688 Preprocessor1_Model1\n2 rsq     standard   0.359    10 0.0164  Preprocessor1_Model1\n\nWe can visualize these results by comparing the predicted rating with\nthe true rating:\n\n\nbind_rows(\n  collect_predictions(svm_rs) %>%\n    mutate(mod = \"SVM\"),\n  collect_predictions(ranger_rs) %>%\n    mutate(mod = \"ranger\")\n) %>%\n  ggplot(aes(rating, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray50\", size = 1.2) +\n  geom_jitter(width = 0.4, alpha = 0.4) +\n  facet_wrap(vars(mod)) +\n  coord_fixed() + hrbrthemes::theme_ipsum()\n\n\n\n\nChoosing a model\nNeither of these prediction models look great, judging by their rsq\nvalues and the general prediction. However, we can probably use the SVM\nmodel for further analysis since it doesn’t take as long as the RF\nmodel. The function last_fit() fits one final time on the training data\nand evaluates on the testing data.\nThis is the first time we have used the testing\ndata.\n\n\nfinal_fitted <- last_fit(svm_wf, choco_split)\ncollect_metrics(final_fitted) ## metrics evaluated on the *testing* data\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.381 Preprocessor1_Model1\n2 rsq     standard       0.348 Preprocessor1_Model1\n\nAgain the results don’t look particularly great, but its just a\npractise run.\nNow the “final_fitted” object can be used to predict the ratings for\neverything in the testing data.\nThis is done by using the workflow to predict the choco_test\ndata.\n\n\nfinal_wf <- extract_workflow(final_fitted)\npredict(final_wf, choco_test[55, ])\n\n\n# A tibble: 1 × 1\n  .pred\n  <dbl>\n1  3.51\n\nNote: You can save this fitted final_wf object to use later with new\ndata, for example with readr::write_rds().\nRating Bais Visualization\nWe can now directly visualize the baises for each of the ‘token’ or\nterm and how they affect the rating of a particular chocolate. This is\ndone from the final_fitted object\n\n\nextract_workflow(final_fitted) %>%\n  tidy() %>% # make a table\n  filter(term != \"Bias\") %>% # remove biases\n  group_by(estimate > 0) %>%\n  slice_max(abs(estimate), n = 10) %>% \n  ungroup() %>%\n  mutate(term = str_remove(term, \"tfidf_most_memorable_characteristics_\")) %>%\n  ggplot(aes(estimate, fct_reorder(term, estimate), fill = estimate > 0)) +\n  geom_col(alpha = 0.8) +\n  scale_fill_discrete(labels = c(\"low ratings\", \"high ratings\")) +\n  labs(y = NULL, fill = \"More from...\") +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nWe see what we noticed during our EDA, i.e. the words like off,\nbitter, chemical heavily turn the rating negative/low while words like\ncreamy, cocoa, complex etc. tend to be associated with higher rated\nchocolates.\n\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-18/readme.md↩︎\nhttps://juliasilge.com/blog/↩︎\nGeneral trend observed, and in no\nways indicative of the true rating of each chocolate.↩︎\nRandom forest in remote sensing: A\nreview of applications and future directions by Mariana Belgiu↩︎\nhttps://www.tmwr.org/pre-proc-table.html↩︎\n",
    "preview": "posts/tidymodels-svm-random-forests/images/logo.svg",
    "last_modified": "2022-05-22T13:11:10+05:30",
    "input_file": {}
  },
  {
    "path": "posts/vscode-vs-rstudio-worth-the-switch/",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "description": "Does Microsoft’s flagship code editor hold up against the old favorite?",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "VSCode",
      "Review",
      "Long-read"
    ],
    "contents": "\n\nContents\nRStudio vs VSCode -\nShould you switch?\nIf you\nare only interested in my conclusion or the TLDR; :\nInstallation\nSetting up the editor\nWorking\nwith R code\nRunning Code\nA Note\nAbout Quarto\nWhat I love about R in\nVSCode\nWhat I don’t love about R in\nVSCode\nThings I hope to try soon\nwith VScode\nConclusion\nFurther\nReading\n\n\n\nRStudio vs VSCode -\nShould you switch?\nRStudio is the preferred IDE/Editor for running R code for most\npeople, but VSCode has rapidly become one of the most popular code\neditors for a huge number of other languages. This writeup is for people\nwho are very familiar with RStudio and want to see how the two compare.\nSo, I will not be talking a lot about RStudio because for that reason,\nbut going into the strengths and weaknesses of VSCode compared to\nRStudio.\nFurther, I this is a very brief overview comparing the community\n(free) versions of both editors, I cannot speak to the R-Studio paid\neditions.\nI am a low-intermediate level programmer who doesn’t use R as much as\nsome people do but whenever I do, RStudio is my preferred editor of\nchoice. For everything else (Python, Jupyter, HTML, CSS, JS,etc ), I\ntend to use VSCode. I tried VSCode for R because I wanted to see if I\ncould use a single code editor for everything I do.\nRStudio is an outstanding piece of software and has never given me\nany major problems during my workflows, this was just an experiement to\nfind a one-size-fits all code editor/IDE.\nIf you\nare only interested in my conclusion or the TLDR; :\nNo, I will not be switching to VSCode for R - FOR NOW. But I love how\ngreat R looks and works on it already.\nNow that we’re done with that, lets get into some of the details:\nInstallation\nAssuming you already have R installed on your system, Installing\nVSCode and R is very straightforward;\ndownload the .pkg or .exe file and install the code editor;\ninstall the languageserver package for R\n(install.packages(“languageserver”)\ninstall the R extension from the markeplace (https://marketplace.visualstudio.com/items?itemName=REditorSupport)\nand you’re ready to start coding.\nOptional: You can install a better terminal alternative like radian,\na debugger and a better plotviewer like httpgd because the builtin\nplotviewer for VSCode isn’t a particualrly good one.\nSetting up the editor\nYou can now start working with code but there are potentially times\nyou could run into errors while running your code.\nTo eliminate any potential errors/code not work, you must make sure\nthe R path is specified in the VSCode preferences (cmd+shift+p on mac)\nand search for R path. Finding the path is simple, open R and type:\nR.home(“bin”) and copy the output path.\nAnother somewhat major tweak you would want to do to the editor:\nSome simple shortcuts like the pipe operator doesn’t work out of the\nbox, so adding a shortcut for “%>%” or “|>” depending on your\npreference can be done using the following method:\nhttps://stackoverflow.com/questions/58731362/how-do-i-had-an-alias-for-magrittr-pipe-from-r-in-vscode\nHow do I had an alias for magrittr pipe from R in vscode - Stack\nOverflow\nWorking with R code\nHere is a comparison of my two setups side by side with the same file\nand environment:\n\n\nworking with R is very similar to a regular RStudio IDE,\nR-extension provides the same support for\ninstalling packages,\nplots and plot viewer\ngrobal enviroment\ndatasets\nplots\nlists, variables, etc.\n\nchecking loaded packages etc.\nTerminal and console position can be modified to make get the input\nand output panes next to each other, something you cannot do well in\nR-Studio. So VSCode feels very familiar and running code is highly\nintuitive.\nRunning Code\nRunning a piece of code is again, very similar to RStudio. A run file\nbutton is located at the top of the panel and individual lines of code\ncan be run using the cmd+return shortcut on mac. The shortcut can be\neasily modified similar to how the pipe operator shortcut is modified\nabove.\nIn terms of running and getting results for plain R code and .R\nfiles; VSCode is a great alternative for a lot of people including me.\nSo if you’re like me and you use R mainly for statistical analysis, data\nvisualization and data analysis; give VSCode a try.\nWorking with Notebooks and Rmd:\nThis is where VSCode falls so much behind RStudio it becomes a one\nhorse race. The notebook support in VSCode is in a single word -\nterrible. Running code chunks in a .Rmd notebook feels and looks dated.\nMy existing notebooks created in RStudio were buggy in VSCode despite\nnot showing any errors in RStudio. Creating new notebooks is unintuitive\nand still a long way behind RStudio in terms of the overall feel to\nit.\nI have searched for solutions, including globally installing pandoc,\nknitr and rmarkdown etc and trying multiple versions of R and the\npackages mentioned previously to no avail. This is one of the biggest\nreasons I am not completely switching to VSCode just yet.\nA Note About Quarto\nI recently started playing around with Quarto, which looks like the\nnatural successor to Rmarkdown. So far, I have only used Quarto in\nRStudio and am a big fan of the way it looks and works. I have yet to\ntry it on VSCode, so I cannot comment on it.\nWhat I love about R in\nVSCode\nSpeed - VSCode is quick to load up, fast and snappy when the code\nis running. I did not do any analysis on the times it took to load a\npiece of code on one vs the other but VSCode ‘feels’ quicker.\nOne stop shop for R, Python, HTML, CSS, and dozens other\nlanguages - and does it much better than RStudio\nMULTIPLE R SESSIONS!!! - Such a great feature when you’re trying\nto develop several related projects at the same time.\nCode Refactoring - much better than RStudio\nColor picker and color blocksFor people like me who work with a\nlot of data visualizations, the ability to see the colors and use the\ncolor blocks to pick colors seamlessly is a gamechanger.\n\nLanguage server - document outline - Navigating through a long\nline of code (and multiple R files) is painless\nIntellisense and Code Completion - faster, more user\nfriendly\n\nHelp and documentation: Same information, much more streamlined\n\nLiveshare and working with teams\nCode Snippets are easier to use and set up than RStudio\nMuch easier git and version control integration\nFunctions are handled brilliantly. Hover over them and you get\nall kinds of information and callbacks.\nCode organization: Code sections allow for folding of code, makes\nfo easier code reading and naviation and cleaner code overall.\n\nCustomizable and make it your own by editing the json files inside\nVSCode.\nSo VSCode has some amazing features that could really tempt an\nRStudio user.\nWhat I don’t love about R in\nVSCode\nR-studio works great out of the box, no need to install extensions\nwhile VSCode setup for R can be tricky and can sometimes involve trail\nand error\nR-templates inside R studio are amazing, and easy to find and\nuse\nDataviewer in RStudio feels a lot better than VScode, especially\nwhen working with a large dataset\nEven though I use VSCode for Python, HTML, CSS etc, coding R in it,\nespecially for the first few hours feels very odd and in a way\nunfamiliar despite the very familiar overall IDE organization\nNOTEBOOK SUPPORT - If you work with rmd more than r, DO NOT SWITCH.\nRunning chunks of code is very clunky and poor\nKnitting notebooks doesnt work well at all, and can lead to errors a\nlot of the time.\nPlot viewers are still not as good as RStudio\nTerminal will sometimes throw errors in VSCode that you wouldn’t\nfind for the same piece of code in RStudio\nRStudio just feels a lot more tailored to a data\nscientist/statistical analysis person, while VSCode can feel like a\ndeveloper-centric code editor\nFeels a lot more tailored to a data scientist/statistical\nanalysis\nThings I hope to try soon\nwith VScode\nQuarto\nRblogdown and Rbookdown\nMore Debugging\nRadian terminal\nConclusion\nDo I love R in VSCode so far? Absolutely\nEnough to make me want to switch? Not yet.\nWhy? Notebooks\nWill I switch soon? Hopefully yes.\nIf\nyou liked this, and have any suggestions for me; find me on twitter: https://twitter.com/karat_sidhu\nFurther Reading\nA great video and blog by Kun Ren on the subject : https://youtu.be/9xXBDU2z_8Y\nA blog on the same subject: https://renkun.me/2019/12/11/writing-r-in-vscode-a-fresh-start/\nRunning R in VSCode: https://www.infoworld.com/article/3625488/how-to-run-r-in-visual-studio-code.html\nInsalling R in VSCode: https://code.visualstudio.com/docs/languages/r\nBonus: Quarto vs RMD: https://yihui.org/en/2022/04/quarto-r-markdown/\nThis writeup is also available on Towards Data Science\nVSCode\nvs RStudio Worth the switch? | Karat Sidhu | Towards Data Science\n(medium.com)\n\n\n\n",
    "preview": "posts/vscode-vs-rstudio-worth-the-switch/images/logo.svg",
    "last_modified": "2022-05-22T10:12:52+05:30",
    "input_file": {}
  },
  {
    "path": "posts/looking-at-eurovision-winners-with-gt/",
    "title": "Eurovision winners with gt() and gtextras()",
    "description": "Using gt() package to make a great looking table.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-17",
    "categories": [
      "TidyTuesday",
      "Data-Viz",
      "TidyVerse"
    ],
    "contents": "\n\nContents\nLoading\nLibraries\nload data\nCleaning\nData\nJust the finale winners\nData to make the table\nMaking\nthe gt() table\n\nMaking a table using the gt and gtextras package in R to look at the\nEurovision winners from 2004 to 2022.\nLoading Libraries\n\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\n\n\n\nload data\n\n\neurovision <-\n  readr::read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-17/eurovision.csv\"\n  )\n\n\n\nCleaning Data\n\n\neurovision <- eurovision |>\n  select(\n    -c(\n      event_url,\n      artist_url,\n      country_emoji,\n      rank_ordinal,\n      running_order,\n      qualified,\n      event\n    )\n  )\n\n\n\nJust the finale winners\n\n\neurovision <- eurovision |>\n  filter(section == \"grand-final\") |>\n  group_by(year) |>\n  mutate(average_points = mean(total_points)) |>\n  ungroup()\n\n\n\nData to make the table\n\n\neurovision_table <- eurovision |>\n  group_by(artist_country) |>\n  filter(section == \"grand-final\") |>\n  filter(winner == \"TRUE\") |>\n  select(-c(section, rank, winner)) |>\n  select(year, everything())\n\n\n\nMaking the gt() table\n\n\ntable  <- eurovision_table |>\n  gt() |>\n  gt_color_rows(total_points:average_points,\n                type = \"discrete\",\n                palette = \"ggsci::orange_material\",\n  ) |>\n  gt_img_rows(columns = image_url,\n              img_source = \"web\",\n              height = 30) |>\n  gt_theme_538() |>\n  tab_header(\n    title = \"Eurovision Song Contest Winners 2004 - 2022\",\n    subtitle = \"Winners of the Eurovision Song contest grand finales from 2004 to 2022. An average of 26 countries participated\n    in the song contest and the following countries won after advancing to the final rounds. The table details the winning artists\n    and the total points for the corresponding winners; in addition total scores for each winner and the average score for that year.\n    Data grouped by country.\"\n  ) |>\n  tab_source_note(source_note = md(\n    glue::glue(\n      \"Data : {fontawesome::fa('twitter')} Tanya Shapiro | Graphic : {fontawesome::fa('github')} github.com/SidhuK\"\n    )\n  )) |>\n  tab_footnote(footnote = \"Highest Points by a winner\",\n               locations = cells_row_groups(groups = \"Portugal\")) |>\n  tab_style(locations = cells_title(groups = 'title'), # format the main title\n            style = list(\n              cell_text(\n                font = google_font(name = 'Bebas Neue'),\n                size = px(60),\n                color = 'indianred',\n                weight = 700\n              )\n            )) |>\n  gt_merge_stack(col1 = host_city,\n                 col2 = host_country) |> # trim the table a bit\n  gt_merge_stack(col1 = artist,\n                 col2 = song) |>\n  tab_options(table.background.color = \"#f1ebda\",\n              column_labels.background.color = \"#f1ebda\") # set the bg color\ntable\n\n\n\nEurovision Song Contest Winners 2004 - 2022\n    Winners of the Eurovision Song contest grand finales from 2004 to 2022. An average of 26 countries participated\n    in the song contest and the following countries won after advancing to the final rounds. The table details the winning artists\n    and the total points for the corresponding winners; in addition total scores for each winner and the average score for that year.\n    Data grouped by country.\n    year\n      host_city\n      artist\n      image_url\n      total_points\n      average_points\n    Ukraine\n    2022\nTurin\nItaly\nKalush Orchestra\nStefania\n\n631\n185.600002016\nStockholm\nNetherlands\nJamala\nZitti E Buoni\n\n534\n187.384622004\nIstanbul\nIsrael\nRuslana\nArcade\n\n280\n87.00000Italy\n    2021\nRotterdam\nPortugal\nMåneskin\nTOY\n\n524\n174.00000Netherlands\n    2019\nTel Aviv\nUkraine\nDuncan Laurence\nAmar Pelos Dois\n\n498\n182.92308Israel\n    2018\nLisbon\nSweden\nNetta\n1944\n\n529\n191.84615Portugal1\n    2017\nKyiv\nAustria\nSalvador Sobral\nHeroes\n\n758\n187.38462Sweden\n    2015\nVienna\nDenmark\nMåns Zelmerlöw\nRise Like a Phoenix\n\n365\n85.925932012\nBaku\nSweden\nLoreen\nOnly Teardrops\n\n372\n93.69231Austria\n    2014\nCopenhagen\nAzerbaijan\nConchita Wurst\nEuphoria\n\n290\n82.53846Denmark\n    2013\nMalmö\nGermany\nEmmelie de Forest\nRunning Scared\n\n281\n87.00000Azerbaijan\n    2011\nDüsseldorf\nNorway\nEll/Nikki\nSatellite\n\n221\n99.76000Germany\n    2010\nOslo\nRussia\nLena\nFairytale\n\n246\n90.48000Norway\n    2009\nMoscow\nSerbia\nAlexander Rybak\nBelieve\n\n387\n97.44000Russia\n    2008\nBelgrade\nFinland\nDima Bilan\nMolitva\n\n272\n99.76000Serbia\n    2007\nHelsinki\nGreece\nMarija Šerifović\nHard Rock Hallelujah\n\n268\n101.50000Finland\n    2006\nAthens\nUkraine\nLordi\nMy Number One\n\n292\n91.83333Greece\n    2005\nKyiv\nTurkey\nHelena Paparizou\nWild Dances\n\n230\n94.25000Data :  Tanya Shapiro | Graphic :  github.com/SidhuK\n    1 Highest Points by a winner\n    \n\n\n\n\n",
    "preview": "posts/looking-at-eurovision-winners-with-gt/images/logo.png",
    "last_modified": "2022-05-22T10:11:10+05:30",
    "input_file": {},
    "preview_width": 770,
    "preview_height": 301
  },
  {
    "path": "posts/new-york-times-bestsellers/",
    "title": "New York Times Bestsellers",
    "description": "Working with geom_jitter and ggrepel to create a great looking plot.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-10",
    "categories": [
      "TidyTuesday",
      "Data-Viz",
      "TidyVerse"
    ],
    "contents": "\n\nContents\nLoading\nLibraries\nLoading Data\nData Cleaning and basic\nEDA\nMaking the\nplot\n\nLooking at the New York Times Bestsellers by Decade; data by\nPost45\nLoading Libraries\n\n\nlibrary(tidyverse)\nlibrary(showtext)\nshowtext_opts(dpi = 450)\nshowtext_auto(enable = TRUE)\nlibrary(ggtext)\nlibrary(ggrepel)\nfont_add_google(family = \"Roboto\", name = \"Roboto\")\nfont_add_google(family = \"Outfit\", name = \"Outfit\")\n\n\n\nLoading Data\n\n\nnyt_titles <-\n    readr::read_tsv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv\")\nnyt_full <-\n    readr::read_tsv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_full.tsv\")\ncolors_legend <- c(\n    \"#FFB327\",\n    \"#08748f\",\n    \"#4F607C\",\n    \"#2d6554\",\n    \"#8E038E\",\n    \"#5A6D87\",\n    \"#000000\",\n    \"#725050\",\n    \"#542ea5\",\n    \"#304d30\",\n    \"#8E038E\"\n)\n\n\n\nData Cleaning and basic EDA\n\n\nnytitiles_weeks <- nyt_titles %>%\n    mutate(\n        decade = (year %/% 10) * 10\n    ) %>%\n    group_by(decade) %>%\n    slice(which.max(total_weeks)) %>%\n    mutate(title_new = paste(title, \"(\", total_weeks, \"Weeks )\"))\nnytitiles_weeks\n\n\n# A tibble: 10 × 10\n# Groups:   decade [10]\n      id title          author  year total_weeks first_week debut_rank\n   <dbl> <chr>          <chr>  <dbl>       <dbl> <date>          <dbl>\n 1   481 ANTHONY ADVER… Herve…  1933          86 1933-07-03          1\n 2  6157 THE ROBE       Lloyd…  1942         111 1942-11-09          5\n 3  4749 THE CAINE MUT… Herma…  1951         123 1951-04-22         13\n 4  6887 TO KILL A MOC… Harpe…  1960          98 1960-08-07         14\n 5  2363 ILLUSIONS      Richa…  1977          80 1977-06-19         10\n 6  4679 THE BONFIRE O… Tom W…  1987          56 1987-11-08          1\n 7  3343 OH, THE PLACE… Dr. S…  1990         178 1990-02-25         14\n 8  4918 THE DA VINCI … Dan B…  2003         165 2003-04-06          9\n 9   381 ALL THE LIGHT… Antho…  2014         132 2014-05-25          2\n10   414 AMERICAN DIRT  Jeani…  2020          34 2020-02-09         12\n# … with 3 more variables: best_rank <dbl>, decade <dbl>,\n#   title_new <chr>\n\nMaking the plot\n\n\nnyt_titles %>%\n    mutate(\n        decade = (year %/% 10) * 10\n    ) %>%\n    ggplot(aes(\n        x = as.factor(reorder(decade, -decade)),\n        y = total_weeks,\n        group = decade,\n        color = as.factor(decade)\n    )) +\n    geom_jitter(width = 0.25, alpha = 0.2, size = 0.85) +\n    geom_text_repel(aes(\n        label = title_new,\n        family = \"Outfit\"\n    ),\n    size = 4.5,\n    data = nytitiles_weeks, alpha = 0.9,\n    box.padding = 0.35\n    ) +\n    geom_point(\n        data = nytitiles_weeks,\n        aes(\n            x = as.factor(decade),\n            y = total_weeks\n        ),\n        size = 2\n    ) +\n    theme_minimal() +\n    labs(\n        title = \"NYTIMES BESTSELLERS\",\n        subtitle = \"**Bestsellers; by Decade.** <br>Looking at the number of weeks each book stayed on the NYTimes bestsellers<br> list every decade. Most popular bestselling book with the most weeks on the <br> list is labelled for each decade.\",\n        caption = \"Data: Post45 | Graphic: github.com/SidhuK\"\n    ) +\n    ylab(\"Weeks\") +\n    theme(\n        legend.position = \"none\",\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.title = element_markdown(family = \"Cookie\", size = 10),\n        plot.subtitle = element_markdown(family = \"Outfit\", size = 18),\n        plot.caption = element_markdown(family = \"Outfit\", size = 12),\n        axis.title.y = element_blank(),\n        axis.title.x = element_text(size = 18),\n        axis.text = element_text(size = 14),\n    ) +\n    scale_color_manual(\n        values = colors_legend,\n        name = NULL\n    ) +\n    coord_flip()\n\n\n\n\n\n\n\n",
    "preview": "posts/new-york-times-bestsellers/images/logo.png",
    "last_modified": "2022-05-22T10:12:23+05:30",
    "input_file": {},
    "preview_width": 1080,
    "preview_height": 264
  },
  {
    "path": "posts/making-streamgraphs-in-r/",
    "title": "Making streamgraphs in R",
    "description": "TidyTuesday May 03,2022 ; Power and Energy Capacity visualization.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-05-03",
    "categories": [
      "TidyTuesday",
      "Data-Viz",
      "TidyVerse"
    ],
    "contents": "\n\nContents\nLoading\nLibraries\nLoading and cleaning Data\nMaking the\nplot\nVisit my\nTidyTuesday Repo for a better look.\n\n\nTidyTuesday Dataset for Week of May 03, 2022. The dataset featured\nthe hidden gems files from Berkeley Lab/Dept of Energy.\nLoading Libraries\n\n\nlibrary(ggstream)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(showtext)\nshowtext_opts(dpi = 450)\nshowtext_auto(enable = TRUE)\nlibrary(ggtext)\nfont_add_google(family = \"Bangers\", name = \"Bangers\")\n\n\n\nLoading and cleaning Data\n\n\ncapacity <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/capacity.csv'\n  )\nwind <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/wind.csv'\n  )\nsolar <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/solar.csv'\n  )\naverage_cost <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/average_cost.csv'\n  )\ncapacity <- capacity %>%\n  mutate(across(where(anyNA), ~ replace_na(., 0)))\ncapacity <- capacity %>%\n  mutate(prior = standalone_prior + hybrid_prior) %>%\n  mutate(new = standalone_new + hybrid_new)\n\n\n\nMaking the plot\n\n\ncolors_legend <- c(\"#FFB327\",\n                   \"#D1F1F9\",\n                   \"#4F607C\",\n                   \"#c5b689\",\n                   \"#8E038E\",\n                   \"#5A6D87\",\n                   \"#000000\")\nplot1 <- ggplot(capacity, aes(year, prior, fill = type)) +\n  geom_stream(\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 1.25,\n    sorting = \"onset\"\n  ) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 0.09,\n    sorting = \"onset\"\n  ) +\n  scale_fill_manual(values = colors_legend,\n                    name = NULL) +\n  scale_x_continuous(breaks = c(2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)) +\n  theme_minimal(base_family = \"Bangers\") +\n  theme(\n    plot.background = element_rect(fill = \"grey84\", color = NA),\n    panel.grid = element_blank(),\n    axis.title = element_blank(),\n    axis.text.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.text = element_text(color = \"grey40\", size = 14),\n    legend.box.margin = margin(t = 30),\n    legend.background = element_rect(color = \"grey40\",\n                                     size = .3,\n                                     fill = \"grey95\"),\n    legend.key.height = unit(.25, \"lines\"),\n    legend.key.width = unit(2.5, \"lines\"),\n    plot.margin = margin(rep(20, 4))\n  ) +\n  labs(title = \"Prior Generation Capacity (Gigawatts)\") +\n  theme(plot.title = element_text(hjust = 0.5))\nplot1\n\n\n\nplot2 <- ggplot(capacity, aes(year, new, fill = type)) +\n  geom_stream(\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 1.25,\n    sorting = \"onset\"\n  ) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 0.02,\n    sorting = \"onset\"\n  ) +\n  scale_fill_manual(values = colors_legend,\n                    name = NULL) +\n  scale_x_continuous(breaks = c(2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)) +\n  theme_minimal(base_family = \"Bangers\") +\n  theme(\n    plot.background = element_rect(fill = \"grey84\", color = NA),\n    panel.grid = element_blank(),\n    axis.title = element_blank(),\n    axis.text.y = element_blank(),\n    legend.position = \"none\",\n    plot.margin = margin(rep(20, 4))\n  ) +\n  labs(title = \"New Generation Capacity (Gigawatts)\") +\n  theme(plot.title = element_text(hjust = 0.5))\nplot_final <- plot1 / plot2 +\n  plot_annotation(\n    title = 'Power Generation Capacity',\n    subtitle = 'The shreamcharts describe the power generation from various sources (solar, nuclear, wind, etc) along with <br>\n    their capacity over the years. The graphs are separated into New and Prior Generation <br> In Gigawatts. <br> ',\n    caption = 'Data: Berkeley Lab | Graphic: Github.com/SidhuK ',\n    theme = theme(\n      plot.title = element_text(size = 35, hjust = 0.5),\n      plot.subtitle = element_markdown(size = 15, hjust = 0.5),\n      plot.caption = element_text(size = 11, hjust = 0.5),\n      plot.background = element_rect(fill = \"grey84\", color = NA)\n    )\n  ) &\n  theme(text = element_text('Bangers'))\n\n\n\n\n\nplot_final\n\n\n\n\nVisit my\nTidyTuesday Repo for a better look.\n\n\n\n",
    "preview": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-21T10:35:29+05:30",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/amazon-bestsellers-data-analysis/",
    "title": "Amazon Bestsellers Data Analysis",
    "description": "KMeans and Exploratory Data Analysis.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-04-26",
    "categories": [
      "Algorithm",
      "Data-Analysis",
      "K-Means"
    ],
    "contents": "\n\nContents\nAmazon\nBestsellers\nLoading\nlibraries\nLoading Data\nExploring\nthe Data\nColumn Names\nChecking for missing\nValues\nData at a\nglance\nData\nDistribution\nall\ndata formats look good, no conversion needed for now\n\nData\nVisualization\nGenres\nBestsellers distribution by\ngenre\n\nGenres through the years\nBestsellers\ndistribution by genre by year\n\nCorrelation Analysis\nIs\nthere any correlation seen between the different numerical\nvariables?\n\nUser\nReviews vs Year\nMost\nreviewed books\nAre the number of\nreviews changing by year?\nLinear Model\nData Viz\n\n\nReviews Distribution\nUser\nRatings vs Year\nHighest\nRated Books\nAre users\nrating the bestsellers differently by year?\nLinear\nModel\nData Viz\n\n\nRatings Distribution\nRatings vs\nReviews\nPrice\nIs there\na substantial change in price over the years?\nLinear\nModel\n\n\nPrice\nDistribution\nMost\nExpensive Books\n\nAuthors\nMost Instances on the\nbest seller list\nWhat\nwere their books?\n\nBooks\nMost instances on the\nbest sellers list\n\nK -\nMeans Clustering\nHierarchical clustering\n\nAmazon Bestsellers\nLoading libraries\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(skimr))\nsuppressPackageStartupMessages(library(corrplot))\nsuppressPackageStartupMessages(library(RColorBrewer))\nLoading Data\nbestsellers <- read.csv(\"bestsellers with categories.csv\")\nhead(bestsellers, 2)\n##                            Name       Author User.Rating Reviews Price Year\n## 1 10-Day Green Smoothie Cleanse     JJ Smith         4.7   17350     8 2016\n## 2             11/22/63: A Novel Stephen King         4.6    2052    22 2011\n##         Genre\n## 1 Non Fiction\n## 2     Fiction\nstr(bestsellers)\n## 'data.frame':    550 obs. of  7 variables:\n##  $ Name       : chr  \"10-Day Green Smoothie Cleanse\" \"11/22/63: A Novel\" \"12 Rules for Life: An Antidote to Chaos\" \"1984 (Signet Classics)\" ...\n##  $ Author     : chr  \"JJ Smith\" \"Stephen King\" \"Jordan B. Peterson\" \"George Orwell\" ...\n##  $ User.Rating: num  4.7 4.6 4.7 4.7 4.8 4.4 4.7 4.7 4.7 4.6 ...\n##  $ Reviews    : int  17350 2052 18979 21424 7665 12643 19735 19699 5983 23848 ...\n##  $ Price      : int  8 22 15 6 12 11 30 15 3 8 ...\n##  $ Year       : int  2016 2011 2018 2017 2019 2011 2014 2017 2018 2016 ...\n##  $ Genre      : chr  \"Non Fiction\" \"Fiction\" \"Non Fiction\" \"Fiction\" ...\nExploring the Data\nColumn Names\nbestsellers %>% colnames()\n## [1] \"Name\"        \"Author\"      \"User.Rating\" \"Reviews\"     \"Price\"      \n## [6] \"Year\"        \"Genre\"\nChecking for missing Values\nany(is.na(bestsellers))\n## [1] FALSE\nData at a glance\nunique(bestsellers$Year)\n##  [1] 2016 2011 2018 2017 2019 2014 2010 2009 2015 2013 2012\nunique(bestsellers$Genre)\n## [1] \"Non Fiction\" \"Fiction\"\nData Distribution\nskim(bestsellers)\nName\nbestsellers\nNumber of rows\n550\nNumber of columns\n7\n_______________________\n\nColumn type frequency:\n\ncharacter\n3\nnumeric\n4\n________________________\n\nGroup variables\nNone\nData summary\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nName\n0\n1\n4\n121\n0\n351\n0\nAuthor\n0\n1\n2\n34\n0\n248\n0\nGenre\n0\n1\n7\n11\n0\n2\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nUser.Rating\n0\n1\n4.62\n0.23\n3.3\n4.5\n4.7\n4.80\n4.9\n▁▁▁▂▇\nReviews\n0\n1\n11953.28\n11731.13\n37.0\n4058.0\n8580.0\n17253.25\n87841.0\n▇▂▁▁▁\nPrice\n0\n1\n13.10\n10.84\n0.0\n7.0\n11.0\n16.00\n105.0\n▇▁▁▁▁\nYear\n0\n1\n2014.00\n3.17\n2009.0\n2011.0\n2014.0\n2017.00\n2019.0\n▇▅▅▅▅\nall\ndata formats look good, no conversion needed for now\nData Visualization\nGenres\nBestsellers distribution by\ngenre\nbestsellers %>%\n  group_by(Genre) %>%\n  summarise(books = n()) %>%\n  ggplot(aes(x = \"\", y = books, fill = Genre)) +\n  geom_bar(stat = \"identity\",\n           width = 1,\n           color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  scale_fill_manual(values = c(\"#ff9900\", \"#000000\"))\n\nGenres through the years\nBestsellers\ndistribution by genre by year\nbestsellers %>%\n  group_by(Year, Genre) %>%\n  summarise(sum = n()) %>%\n  ggplot(aes(\n    x = Year,\n    y = sum,\n    fill = Genre\n  )) +\n  geom_col() +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2009, 2019, by = 1)) +\n  scale_fill_manual(values = c(\"#ff9900\", \"#000000\")) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )\n## `summarise()` has grouped output by 'Year'. You can override using the\n## `.groups` argument.\n\nCorrelation Analysis\nIs\nthere any correlation seen between the different numerical\nvariables?\nbestsellers %>%\n  select(User.Rating, Reviews, Price, Year) %>%\n  cor() %>%\n  corrplot::corrplot(\n    type = \"lower\",\n    order = \"hclust\",\n    method = \"color\",\n    addgrid.col = \"darkgray\",\n    outline = T,\n    tl.cex = 1,\n    tl.col = \"black\",\n    col = brewer.pal(n = 6, name = \"RdGy\")\n  )\n\nUser Reviews vs Year\nMost reviewed books\nbestsellers %>% \n  select(Name, Reviews) %>% \n  arrange(desc(Reviews)) %>% \n  head(20) %>% \n  distinct()\n##                                                                                        Name\n## 1                                                                   Where the Crawdads Sing\n## 2                                                                     The Girl on the Train\n## 3                                                                                  Becoming\n## 4                                                                                 Gone Girl\n## 5                                                                    The Fault in Our Stars\n## 6                                                                  The Nightingale: A Novel\n## 7  Fifty Shades of Grey: Book One of the Fifty Shades Trilogy (Fifty Shades of Grey Series)\n## 8                                                                               The Martian\n## 9                                                               All the Light We Cannot See\n## 10                                                                            The Alchemist\n##    Reviews\n## 1    87841\n## 2    79446\n## 3    61133\n## 4    57271\n## 5    50482\n## 6    49288\n## 7    47265\n## 8    39459\n## 9    36348\n## 10   35799\nAre the number of\nreviews changing by year?\nLinear Model\nbestsellers %>% \n  lm(formula = Year ~ Reviews) %>% \n  summary()\n## \n## Call:\n## lm(formula = Year ~ Reviews, data = .)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5523 -2.5991 -0.3255  2.5354  5.6549 \n## \n## Coefficients:\n##              Estimate Std. Error   t value Pr(>|t|)    \n## (Intercept) 2.013e+03  1.861e-01 10816.121  < 2e-16 ***\n## Reviews     7.111e-05  1.112e-05     6.396 3.42e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.056 on 548 degrees of freedom\n## Multiple R-squared:  0.06946,    Adjusted R-squared:  0.06777 \n## F-statistic: 40.91 on 1 and 548 DF,  p-value: 3.423e-10\nData Viz\nbestsellers %>% \n  ggplot(aes(Year, Reviews, color = Genre))+\n geom_jitter()+\n geom_smooth(method = 'lm', formula = y ~ x )+\n  theme_minimal() +\n    scale_color_manual(values = c(\"#ff9900\", \"#000000\")) +\n    scale_x_continuous(breaks = seq(2009, 2019, by = 1))+\n   theme(\n    panel.grid.minor = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )\n\nReviews Distribution\nbestsellers %>% \n  ggplot(aes(Reviews, ..density..)) +\n  geom_histogram(fill = \"#ff9900\", color = 'black',binwidth = 1000) +\n    geom_density(alpha = 0.5, fill = \"#ff9900\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank())\n\nUser Ratings vs Year\nHighest Rated Books\nbestsellers %>% \n  select(Name, User.Rating) %>% \n  arrange(desc(User.Rating)) %>% \n  head(20) %>% \n  distinct()\n##                                                                                               Name\n## 1                                                         Brown Bear, Brown Bear, What Do You See?\n## 2                         Dog Man and Cat Kid: From the Creator of Captain Underpants (Dog Man #4)\n## 3              Dog Man: A Tale of Two Kitties: From the Creator of Captain Underpants (Dog Man #3)\n## 4                  Dog Man: Brawl of the Wild: From the Creator of Captain Underpants (Dog Man #6)\n## 5                           Dog Man: Fetch-22: From the Creator of Captain Underpants (Dog Man #8)\n## 6            Dog Man: For Whom the Ball Rolls: From the Creator of Captain Underpants (Dog Man #7)\n## 7                  Dog Man: Lord of the Fleas: From the Creator of Captain Underpants (Dog Man #5)\n## 8  Goodnight, Goodnight Construction Site (Hardcover Books for Toddlers, Preschool Books for Kids)\n## 9                                                                         Hamilton: The Revolution\n## 10         Harry Potter and the Chamber of Secrets: The Illustrated Edition (Harry Potter, Book 2)\n## 11         Harry Potter and the Goblet of Fire: The Illustrated Edition (Harry Potter, Book 4) (4)\n## 12        Harry Potter and the Prisoner of Azkaban: The Illustrated Edition (Harry Potter, Book 3)\n## 13           Harry Potter and the Sorcerer's Stone: The Illustrated Edition (Harry Potter, Book 1)\n## 14                                                                    Humans of New York : Stories\n## 15                       Jesus Calling: Enjoying Peace in His Presence (with Scripture References)\n##    User.Rating\n## 1          4.9\n## 2          4.9\n## 3          4.9\n## 4          4.9\n## 5          4.9\n## 6          4.9\n## 7          4.9\n## 8          4.9\n## 9          4.9\n## 10         4.9\n## 11         4.9\n## 12         4.9\n## 13         4.9\n## 14         4.9\n## 15         4.9\nAre users\nrating the bestsellers differently by year?\nLinear Model\nbestsellers %>% \n  lm(formula = Year ~ Reviews) %>% \n  summary()\n## \n## Call:\n## lm(formula = Year ~ Reviews, data = .)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5523 -2.5991 -0.3255  2.5354  5.6549 \n## \n## Coefficients:\n##              Estimate Std. Error   t value Pr(>|t|)    \n## (Intercept) 2.013e+03  1.861e-01 10816.121  < 2e-16 ***\n## Reviews     7.111e-05  1.112e-05     6.396 3.42e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.056 on 548 degrees of freedom\n## Multiple R-squared:  0.06946,    Adjusted R-squared:  0.06777 \n## F-statistic: 40.91 on 1 and 548 DF,  p-value: 3.423e-10\nData Viz\nbestsellers %>% \n  ggplot(aes(Year, User.Rating, color = Genre))+\n geom_jitter()+\n geom_smooth(method = 'lm', formula = y ~ x )+\n  theme_minimal() +\n    scale_color_manual(values = c(\"#ff9900\", \"#000000\")) +\n    scale_x_continuous(breaks = seq(2009, 2019, by = 1))+\n   theme(\n    panel.grid.minor = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )\n\nRatings Distribution\nbestsellers %>%\n  ggplot(aes(User.Rating, ..density..)) +\n  geom_histogram(fill = \"#ff9900\",\n                 color = 'black',\n                 binwidth = 0.1) +\n  geom_density(alpha = 0.5, fill = \"#ff9900\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank())\n\nRatings vs Reviews\nggplot(bestsellers) +\n  aes(\n    x = Reviews,\n    y = User.Rating,\n    colour = Genre,\n    size = Reviews\n  ) +\n  geom_jitter(alpha = 0.45) +\n  scale_color_manual(\n    values = c(Fiction = \"#ff9900\",\n    `Non Fiction` = \"#000000\")\n  ) +\n  theme_minimal()\n\nPrice\nIs there\na substantial change in price over the years?\nLinear Model\nbestsellers %>% \n  lm(formula = Year ~ Price) %>% \n  summary()\n## \n## Call:\n## lm(formula = Year ~ Price, data = .)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5439 -2.6449 -0.0944  2.7708  5.6248 \n## \n## Coefficients:\n##               Estimate Std. Error  t value Pr(>|t|)    \n## (Intercept) 2014.58885    0.20945 9618.334  < 2e-16 ***\n## Price         -0.04495    0.01232   -3.648 0.000289 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.13 on 548 degrees of freedom\n## Multiple R-squared:  0.02371,    Adjusted R-squared:  0.02193 \n## F-statistic: 13.31 on 1 and 548 DF,  p-value: 0.0002895\nbestsellers %>% \n  ggplot(aes(Year, Price, color = Genre))+\n geom_jitter()+\n geom_smooth(method = 'lm', formula = y ~ x )+\n  theme_minimal() +\n    scale_color_manual(values = c(\"#ff9900\", \"#000000\")) +\n    scale_x_continuous(breaks = seq(2009, 2019, by = 1))+\n   theme(\n    panel.grid.minor = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )\n\nPrice Distribution\nbestsellers %>%\n  ggplot(aes(Price, ..density..)) +\n  geom_histogram(fill = \"#ff9900\",\n                 color = 'black',\n                 binwidth = 1.3) +\n  geom_density(alpha = 0.5, fill = \"#ff9900\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank())\n\nMost Expensive Books\nbestsellers %>% \n  select(Name, Price) %>% \n  arrange(desc(Price)) %>% \n  head(20) %>% \n  distinct() %>% \n    ggplot(aes(x =reorder(Name, Price), y = Price,\n              fill = ifelse(Price == max(Price), \"red\",\"grey\"))) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#000000\", \"#ff9900\")) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"none\",\n    axis.title.y = element_blank()\n  )\n\nAuthors\nMost Instances on the\nbest seller list\nbestsellers %>% \n  group_by(Author) %>% \n  summarise(count = n()) %>% \n  arrange(desc(count)) %>% \n  head(10) %>% \n  ggplot(aes(x =reorder(Author, count), y = count,\n              fill = ifelse(count == max(count), \"red\",\"grey\"))) +\n  scale_y_continuous(breaks = seq(0, 13, by = 1))+\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#000000\", \"#ff9900\")) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"none\",\n    axis.title.y = element_blank()\n  )\n\nWhat were their books?\nbestsellers %>% \n  filter(Author==\"Jeff Kinney\")\n##                                                  Name      Author User.Rating\n## 1          Cabin Fever (Diary of a Wimpy Kid, Book 6) Jeff Kinney         4.8\n## 2             Diary of a Wimpy Kid: Hard Luck, Book 8 Jeff Kinney         4.8\n## 3       Diary of a Wimpy Kid: The Last Straw (Book 3) Jeff Kinney         4.8\n## 4                 Diary of a Wimpy Kid: The Long Haul Jeff Kinney         4.8\n## 5  Dog Days (Diary of a Wimpy Kid, Book 4) (Volume 4) Jeff Kinney         4.8\n## 6              Double Down (Diary of a Wimpy Kid #11) Jeff Kinney         4.8\n## 7               Old School (Diary of a Wimpy Kid #10) Jeff Kinney         4.8\n## 8                                         The Getaway Jeff Kinney         4.8\n## 9         The Meltdown (Diary of a Wimpy Kid Book 13) Jeff Kinney         4.8\n## 10     The Third Wheel (Diary of a Wimpy Kid, Book 7) Jeff Kinney         4.7\n## 11      The Ugly Truth (Diary of a Wimpy Kid, Book 5) Jeff Kinney         4.8\n## 12       Wrecking Ball (Diary of a Wimpy Kid Book 14) Jeff Kinney         4.9\n##    Reviews Price Year   Genre\n## 1     4505     0 2011 Fiction\n## 2     6812     0 2013 Fiction\n## 3     3837    15 2009 Fiction\n## 4     6540    22 2014 Fiction\n## 5     3181    12 2009 Fiction\n## 6     5118    20 2016 Fiction\n## 7     6169     7 2015 Fiction\n## 8     5836     0 2017 Fiction\n## 9     5898     8 2018 Fiction\n## 10    6377     7 2012 Fiction\n## 11    3796    12 2010 Fiction\n## 12    9413     8 2019 Fiction\nBooks\nMost instances on the\nbest sellers list\nbestsellers %>% \n  group_by(Name) %>% \n  summarise(count = n()) %>% \n  arrange(desc(count)) %>% \n  head(10) %>% \n  ggplot(aes(x =reorder(Name, count), y = count,\n              fill = ifelse(count == max(count), \"red\",\"grey\"))) +\n    scale_y_continuous(breaks = seq(0, 13, by = 1))+\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#000000\", \"#ff9900\")) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"none\",\n    axis.title.y = element_blank()\n  )\n\nK - Means Clustering\ndata_kmeans <- bestsellers[3:6]\ndata_kmeans <- data_kmeans %>% \n  slice(1:20)\nlibrary(factoextra)\n## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\ndata_kmeans <- bestsellers[3:6]\ndata_kmeans <- data_kmeans %>% \n  slice(1:20)\n\ndata_kmeans_scaled <- scale(data_kmeans)\ndata_kmeans <- dist(data_kmeans_scaled)\nfviz_nbclust(data_kmeans_scaled, kmeans,\n             method = \"wss\") + # wss means within sum squares\n  labs(subtitle = \"Elbow Method\")\n\nkmeans_output <- kmeans(data_kmeans_scaled, centers = 3, nstart = 100)\nkmeans_output.clusters <- kmeans_output$cluster\nbestsellers_mini <- bestsellers %>% \n  slice(1:20)\nrownames(data_kmeans_scaled) <- paste(bestsellers_mini$Name, 1:dim(bestsellers_mini)[1], sep=\"_\")\nfviz_cluster(list(data = data_kmeans_scaled, cluster = kmeans_output.clusters)) +\n  theme_minimal() +\n  theme(plot.subtitle = element_text(size = 12,\n                               face = \"italic\"),\n  plot.caption = element_text(size = 9),\n  plot.title = element_text(size = 15,\n                            face = \"bold\")\n) + labs(\n  colour = \"Cluster\",\n  fill = \"Cluster\",\n  shape = \"Cluster\",\n)\n\ntest <- table(kmeans_output.clusters, bestsellers_mini$Name)\nas.data.frame(test)\n##    kmeans_output.clusters\n## 1                       1\n## 2                       2\n## 3                       3\n## 4                       1\n## 5                       2\n## 6                       3\n## 7                       1\n## 8                       2\n## 9                       3\n## 10                      1\n## 11                      2\n## 12                      3\n## 13                      1\n## 14                      2\n## 15                      3\n## 16                      1\n## 17                      2\n## 18                      3\n## 19                      1\n## 20                      2\n## 21                      3\n## 22                      1\n## 23                      2\n## 24                      3\n## 25                      1\n## 26                      2\n## 27                      3\n## 28                      1\n## 29                      2\n## 30                      3\n## 31                      1\n## 32                      2\n## 33                      3\n## 34                      1\n## 35                      2\n## 36                      3\n## 37                      1\n## 38                      2\n## 39                      3\n## 40                      1\n## 41                      2\n## 42                      3\n## 43                      1\n## 44                      2\n## 45                      3\n## 46                      1\n## 47                      2\n## 48                      3\n## 49                      1\n## 50                      2\n## 51                      3\n## 52                      1\n## 53                      2\n## 54                      3\n## 55                      1\n## 56                      2\n## 57                      3\n##                                                                                                                     Var2\n## 1                                                                                          10-Day Green Smoothie Cleanse\n## 2                                                                                          10-Day Green Smoothie Cleanse\n## 3                                                                                          10-Day Green Smoothie Cleanse\n## 4                                                                                                      11/22/63: A Novel\n## 5                                                                                                      11/22/63: A Novel\n## 6                                                                                                      11/22/63: A Novel\n## 7                                                                                12 Rules for Life: An Antidote to Chaos\n## 8                                                                                12 Rules for Life: An Antidote to Chaos\n## 9                                                                                12 Rules for Life: An Antidote to Chaos\n## 10                                                                                                1984 (Signet Classics)\n## 11                                                                                                1984 (Signet Classics)\n## 12                                                                                                1984 (Signet Classics)\n## 13                                                    5,000 Awesome Facts (About Everything!) (National Geographic Kids)\n## 14                                                    5,000 Awesome Facts (About Everything!) (National Geographic Kids)\n## 15                                                    5,000 Awesome Facts (About Everything!) (National Geographic Kids)\n## 16                                                                         A Dance with Dragons (A Song of Ice and Fire)\n## 17                                                                         A Dance with Dragons (A Song of Ice and Fire)\n## 18                                                                         A Dance with Dragons (A Song of Ice and Fire)\n## 19                    A Game of Thrones / A Clash of Kings / A Storm of Swords / A Feast of Crows / A Dance with Dragons\n## 20                    A Game of Thrones / A Clash of Kings / A Storm of Swords / A Feast of Crows / A Dance with Dragons\n## 21                    A Game of Thrones / A Clash of Kings / A Storm of Swords / A Feast of Crows / A Dance with Dragons\n## 22                                                                                        A Gentleman in Moscow: A Novel\n## 23                                                                                        A Gentleman in Moscow: A Novel\n## 24                                                                                        A Gentleman in Moscow: A Novel\n## 25                                                                         A Higher Loyalty: Truth, Lies, and Leadership\n## 26                                                                         A Higher Loyalty: Truth, Lies, and Leadership\n## 27                                                                         A Higher Loyalty: Truth, Lies, and Leadership\n## 28                                                                                             A Man Called Ove: A Novel\n## 29                                                                                             A Man Called Ove: A Novel\n## 30                                                                                             A Man Called Ove: A Novel\n## 31                        A Patriot's History of the United States: From Columbus's Great Discovery to the War on Terror\n## 32                        A Patriot's History of the United States: From Columbus's Great Discovery to the War on Terror\n## 33                        A Patriot's History of the United States: From Columbus's Great Discovery to the War on Terror\n## 34                                                                                               A Stolen Life: A Memoir\n## 35                                                                                               A Stolen Life: A Memoir\n## 36                                                                                               A Stolen Life: A Memoir\n## 37                                                                                      A Wrinkle in Time (Time Quintet)\n## 38                                                                                      A Wrinkle in Time (Time Quintet)\n## 39                                                                                      A Wrinkle in Time (Time Quintet)\n## 40          Act Like a Lady, Think Like a Man: What Men Really Think About Love, Relationships, Intimacy, and Commitment\n## 41          Act Like a Lady, Think Like a Man: What Men Really Think About Love, Relationships, Intimacy, and Commitment\n## 42          Act Like a Lady, Think Like a Man: What Men Really Think About Love, Relationships, Intimacy, and Commitment\n## 43     Adult Coloring Book Designs: Stress Relief Coloring Book: Garden Designs, Mandalas, Animals, and Paisley Patterns\n## 44     Adult Coloring Book Designs: Stress Relief Coloring Book: Garden Designs, Mandalas, Animals, and Paisley Patterns\n## 45     Adult Coloring Book Designs: Stress Relief Coloring Book: Garden Designs, Mandalas, Animals, and Paisley Patterns\n## 46                                                                  Adult Coloring Book: Stress Relieving Animal Designs\n## 47                                                                  Adult Coloring Book: Stress Relieving Animal Designs\n## 48                                                                  Adult Coloring Book: Stress Relieving Animal Designs\n## 49                                                                        Adult Coloring Book: Stress Relieving Patterns\n## 50                                                                        Adult Coloring Book: Stress Relieving Patterns\n## 51                                                                        Adult Coloring Book: Stress Relieving Patterns\n## 52 Adult Coloring Books: A Coloring Book for Adults Featuring Mandalas and Henna Inspired Flowers, Animals, and Paisley…\n## 53 Adult Coloring Books: A Coloring Book for Adults Featuring Mandalas and Henna Inspired Flowers, Animals, and Paisley…\n## 54 Adult Coloring Books: A Coloring Book for Adults Featuring Mandalas and Henna Inspired Flowers, Animals, and Paisley…\n## 55                                                                                                    Alexander Hamilton\n## 56                                                                                                    Alexander Hamilton\n## 57                                                                                                    Alexander Hamilton\n##    Freq\n## 1     0\n## 2     0\n## 3     1\n## 4     1\n## 5     0\n## 6     0\n## 7     0\n## 8     0\n## 9     1\n## 10    0\n## 11    0\n## 12    1\n## 13    0\n## 14    0\n## 15    1\n## 16    0\n## 17    1\n## 18    0\n## 19    1\n## 20    0\n## 21    0\n## 22    0\n## 23    0\n## 24    1\n## 25    0\n## 26    0\n## 27    1\n## 28    0\n## 29    0\n## 30    2\n## 31    0\n## 32    1\n## 33    0\n## 34    1\n## 35    0\n## 36    0\n## 37    0\n## 38    1\n## 39    0\n## 40    1\n## 41    0\n## 42    0\n## 43    0\n## 44    1\n## 45    0\n## 46    0\n## 47    1\n## 48    0\n## 49    0\n## 50    1\n## 51    0\n## 52    0\n## 53    1\n## 54    0\n## 55    0\n## 56    0\n## 57    1\nHierarchical clustering\nres <- hcut(data_kmeans_scaled, k = 4, stand = TRUE)\n# Visualize\nfviz_dend(res, rect = TRUE, cex = 0.3,\n          k_colors = c(\"#00AFBB\",\"#2E9FDF\", \"#E7B800\", \"#FC4E07\")) +\n  theme_minimal()\n## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\n\n\n\n\n",
    "preview": "posts/amazon-bestsellers-data-analysis/images/logo.png",
    "last_modified": "2022-05-22T10:10:22+05:30",
    "input_file": {},
    "preview_width": 467,
    "preview_height": 397
  },
  {
    "path": "posts/netflix-original-movies-eda/",
    "title": "Netflix Original Movies - EDA",
    "description": "Exploratory Data Analysis, and data vis with ggplot2.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2022-04-21",
    "categories": [
      "GGplot2",
      "Data-Analysis"
    ],
    "contents": "\n\nContents\nNetflix Movies\nLoading\nlibraries\nLoading data\nLooking at the data in brief\nType of datatype in each\ncolumn\nDoes the data contain any\nN/A values?\n\nConverting the\n`premier` from chr to datetime\nGetting each year,\nmonth and date separated\nAlso\nadding the corresponding day of the week for each release\n\nWhen the movies were released\nNumber of Movies released\neach year\nNumber of Movies released\neach month\nNumber of Movies\nreleased by date of the month\nNumber of Movies\nreleased each day of the week\n\nMost\nPopular Genres\n5\nMost Popular Genres\n\nMost Popular Languages\n5 Most Popular Languages\n\nIMDB Scores\nHow were most movies rated?\nHighest Rated Movies\nLowest\nRated Movies\n\nRuntime\nHow long are the movies?\nLongest\nMovies\nShortest\nMovies\n\nRuntime vs IMDB-Score\nBasic Statistical Analysis\nLinear\nModels\nCorrelation\nTest\nP-value\nCorrelation Coefficient\n\nThank you!\n\n\nNetflix Movies\nnetflix-intro-netflix.gif (498×278)\n(tenor.comLoading libraries\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(lubridate))\nsuppressPackageStartupMessages(library(showtext))\nshowtext.auto()\n## 'showtext.auto()' is now renamed to 'showtext_auto()'\n## The old version still works, but consider using the new function in future code\nfont_add_google(\"Bebas Neue\", \"Bebas Neue\")\nLoading data\nnetflix <- read.csv(\"NetflixOriginals.csv\")\nLooking at the data in brief\nhead(netflix, 4)\n##             Title                 Genre          Premiere Runtime IMDB.Score\n## 1 Enter the Anime           Documentary    August 5, 2019      58        2.5\n## 2     Dark Forces              Thriller   August 21, 2020      81        2.6\n## 3         The App Science fiction/Drama December 26, 2019      79        2.6\n## 4  The Open House       Horror thriller  January 19, 2018      94        3.2\n##           Language\n## 1 English/Japanese\n## 2          Spanish\n## 3          Italian\n## 4          English\nType of datatype in each\ncolumn\nas.tibble(sapply(netflix, class))\n## Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n## Please use `as_tibble()` instead.\n## The signature and semantics have changed, see `?as_tibble`.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n## # A tibble: 6 × 1\n##   value    \n##   <chr>    \n## 1 character\n## 2 character\n## 3 character\n## 4 integer  \n## 5 numeric  \n## 6 character\nDoes the data contain any\nN/A values?\nany(is.na(netflix))\n## [1] FALSE\nConverting the\n`premier` from chr to datetime\nnetflix <- netflix %>% \n  mutate(Released = mdy(Premiere))\nGetting each year,\nmonth and date separated\nAlso\nadding the corresponding day of the week for each release\nnetflix <- netflix %>% \n  mutate(Year = year(Released)) %>% \n  mutate(Month = month(Released, label = TRUE)) %>% \n  mutate(Date = day(Released)) %>% \n  mutate(Day = wday(Released, label = TRUE, abbr = FALSE))\nWhen the movies were\nreleased\nNumber of Movies released\neach year\nnetflix %>%\n  group_by(Year) %>%\n  summarise(total = n()) %>%\n  ggplot(aes(\n    x = Year,\n    y = total,\n    fill = ifelse(total == max(total), \"red\",\"grey\"))) +\n  geom_col() +\n  labs(title = \"Netflix Movies released each year\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"#2d2d2d\", \"#E50914\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 35,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor  = element_blank(),\n    text = element_text(size = 20))\n\nNumber of Movies released\neach month\nnetflix %>%\n  group_by(Month) %>%\n  summarise(total = n()) %>%\n  ggplot(aes(\n    x = Month,\n    y = total,\n    fill = ifelse(total == max(total), \"red\",\"grey\"))) +\n  geom_col() +\n    labs(title = \"Netflix Movies released each month\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 30,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    text = element_text(size = 20))\n\nNumber of Movies\nreleased by date of the month\nnetflix %>%\n  group_by(Date) %>%\n  summarise(total = n()) %>%\n  ggplot(aes(\n    x = Date,\n    y = total,\n    fill = ifelse(total == max(total), \"red\",\"grey\")))  +\n  geom_col() +\n    labs(title = \"Netflix Movies released by Date of each month\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 30,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank(),\n    text = element_text(size = 20))\n\nNumber of Movies\nreleased each day of the week\nnetflix %>%\n  group_by(Day) %>%\n  summarise(total = n()) %>%\n  ggplot(aes(\n    x = Day,\n    y = total,\n    fill = ifelse(total == max(total), \"red\",\"black\"))) +\n  geom_col() +\n  labs(title = \"Netflix Movies released by day of the week\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 30,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    text = element_text(size = 20))\n\nMost Popular Genres\n5 Most Popular Genres\nnetflix %>% \n  group_by(Genre) %>% \n  summarise(Movies = n()) %>% \n  arrange(desc(Movies)) %>% \n  head(5) %>% \n   ggplot(aes(\n    x = reorder(Genre, -Movies),\n    y = Movies,\n    fill = ifelse(Movies == max(Movies), \"red\", \"black\"))) +\n  geom_col() +\n  labs(title = \"Most Popular Genres\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 30,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    text = element_text(size = 20))\n\nMost Popular Languages\n5 Most Popular Languages\nnetflix %>% \n  group_by(Language) %>% \n  summarise(Movies = n()) %>% \n  arrange(desc(Movies)) %>% \n  head(5) %>% \n   ggplot(aes(\n    x = reorder(Language, -Movies),\n    y = Movies,\n    fill = ifelse(Movies == max(Movies), \"red\", \"black\"))) +\n  geom_col() +\n  labs(title = \"Most Popular Languages\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 30,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    text = element_text(size = 20))\n\nIMDB Scores\nHow were most movies rated?\nnetflix %>%\n  ggplot(aes(x = `IMDB.Score`)) +\n  geom_dotplot(binwidth = 0.1,\n               fill = \"#2d2d2d\",\n               color = \"#e9ecef\") +\n  labs(title = \"IMDB Score Distribution\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 25,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank()\n  ) \n\nHighest Rated Movies\nnetflix %>% \n  arrange(desc(`IMDB.Score`)) %>% \n  head(5) %>% \n   ggplot(aes(\n    x = reorder(`Title`, `IMDB.Score`),\n    y = `IMDB.Score`,\n    fill = ifelse(`IMDB.Score` == max(`IMDB.Score`), \"red\", \"black\"))) +\n  geom_col() +\n  labs(title = \"Highest Rated Movies\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  coord_flip() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 25,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank())\n\nLowest Rated Movies\nnetflix %>% \n  arrange(desc(-`IMDB.Score`)) %>% \n  head(5) %>% \n  ggplot(aes(\n    x = reorder(`Title`, -`IMDB.Score`),\n    y = `IMDB.Score`,\n    fill = ifelse(`IMDB.Score` == min(`IMDB.Score`), \"red\", \"black\"))) +\n  geom_col() +\n  labs(title = \"Lowest Rated Movies\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  coord_flip() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 25,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank())\n\nRuntime\nHow long are the movies?\nnetflix %>%\n  ggplot(aes(x = Runtime)) +\n  geom_dotplot(binwidth = 2.25,\n               fill = \"#2d2d2d\",\n               color = \"#e9ecef\") +\n  labs(title = \"Movie Runtime\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 25,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank()\n  ) \n\nLongest Movies\nnetflix %>% \n  arrange(desc(Runtime)) %>% \n  head(5) %>% \n  ggplot(aes(\n    x = reorder(`Title`, `Runtime`),\n    y = `Runtime`,\n    fill = ifelse(Runtime == max(`Runtime`), \"red\", \"black\"))) +\n  geom_col() +\n  labs(title = \"Longest Movies\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  coord_flip() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 25,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank())\n\nShortest Movies\nnetflix %>% \n  arrange(desc(-Runtime)) %>% \n  head(5) %>% \n  ggplot(aes(\n    x = reorder(`Title`, `Runtime`),\n    y = `Runtime`,\n    fill = ifelse(Runtime == min(`Runtime`), \"red\", \"black\"))) +\n  geom_col() +\n  labs(title = \"Shortest Movies\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  coord_flip() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 25,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    text = element_text(size = 20))\n\nRuntime vs IMDB-Score\nnetflix %>% \n  ggplot(aes(x=`IMDB.Score`, y= Runtime)) +\n  geom_point() +\n  geom_smooth(method = lm,colour = \"#E50914\") +\n  labs(title = \"Runtime vs IMDB Rating\") +\n  theme_minimal() +\n  scale_fill_manual(values = c( \"#2d2d2d\", \"#E50914\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Bebas Neue\",\n      size = 25,\n      colour = \"#E50914\"\n    ),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.x = element_blank())\n## `geom_smooth()` using formula 'y ~ x'\n\nBasic Statistical Analysis\nLinear Models\nmodel <- lm(data = netflix, formula = Runtime ~ `IMDB.Score`)\nsummary(model)\n## \n## Call:\n## lm(formula = Runtime ~ IMDB.Score, data = netflix)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -91.399  -7.439   3.398  14.467 117.195 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  100.849      7.453  13.531   <2e-16 ***\n## IMDB.Score    -1.159      1.174  -0.987    0.324    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27.76 on 582 degrees of freedom\n## Multiple R-squared:  0.001673,   Adjusted R-squared:  -4.283e-05 \n## F-statistic: 0.975 on 1 and 582 DF,  p-value: 0.3238\nCorrelation Test\nres <- cor.test(netflix$Runtime, netflix$IMDB.Score, \n                    method = \"pearson\")\nres\n## \n##  Pearson's product-moment correlation\n## \n## data:  netflix$Runtime and netflix$IMDB.Score\n## t = -0.98744, df = 582, p-value = 0.3238\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.12162699  0.04037194\n## sample estimates:\n##         cor \n## -0.04089629\nP-value\nres$p.value\n## [1] 0.3238393\nCorrelation Coefficient\nres$estimate\n##         cor \n## -0.04089629\nThank you!\nend\n\n\n",
    "preview": {},
    "last_modified": "2022-05-21T17:48:08+05:30",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to karat_codes",
    "description": "Welcome to my new blog, karat_codes.",
    "author": [
      {
        "name": "Karat Sidhu",
        "url": {}
      }
    ],
    "date": "2020-05-21",
    "categories": [
      "misc"
    ],
    "contents": "\nThis is a new blog that I made with distill package in R\nIts based on a custom css theme that I am constantly tweaking.\nSource code for everything available on my github.\ngithub.com/SidhuK\n\n\n\n",
    "preview": {},
    "last_modified": "2022-05-21T11:11:08+05:30",
    "input_file": {}
  }
]
